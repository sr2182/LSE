{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d37af748-1ab6-4a9a-bddf-83690c428e8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import os.path as osp\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import torch\n",
    "from nuscenes.nuscenes import NuScenes\n",
    "from nuscenes.utils.data_classes import RadarPointCloud"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6daa1834-091b-41c2-933a-a3fda0f40923",
   "metadata": {},
   "source": [
    "#### Load the nuScenes dataset (mini-split, in this case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05c45eda-0b4e-4455-9532-6a28401bf87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nusc = NuScenes(version='v1.0-mini', dataroot='./data/sets/nuscenes', verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "999ce686-77ed-4b8c-9d5f-0e9b426fa88e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "404"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nusc.sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e3b2f8-e7d1-4fec-bd8d-74d6431caa73",
   "metadata": {},
   "source": [
    "#### Extract RADAR point cloud from dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20f63dcc-e99c-41ef-8d75-fff794ec871b",
   "metadata": {},
   "outputs": [],
   "source": [
    "radar_dataset = []\n",
    "front_cam_images_datapath = []\n",
    "for i in range(404):\n",
    "    sample_record = nusc.sample[i]\n",
    "    pointsensor_token = sample_record['data']['RADAR_FRONT']\n",
    "    pointsensor = nusc.get('sample_data', pointsensor_token)\n",
    "    pcl_path = osp.join(nusc.dataroot, pointsensor['filename'])\n",
    "    \n",
    "    pc = RadarPointCloud.from_file(pcl_path)\n",
    "    pc = pc.points.T\n",
    "    radar_xy = [[item[0],item[1]] for item in pc]\n",
    "    radar_dataset.append(radar_xy)\n",
    "    # print(pc.shape)  # shape: (N, 18), where N is the number of radar points\n",
    "\n",
    "    my_sample = nusc.get('sample', sample_record['token'])\n",
    "    sensor = 'CAM_FRONT'\n",
    "    cam_front_data = nusc.get('sample_data', my_sample['data'][sensor])\n",
    "    data_path, boxes, camera_intrinsic = nusc.get_sample_data(cam_front_data['token'])\n",
    "    # img = cv2.imread(data_path)\n",
    "    front_cam_images_datapath.append(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065f4fb0-b41f-4c7d-97a7-68b2e50dd2e5",
   "metadata": {},
   "source": [
    "#### Extract bounding box using YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea25733e-cab9-454b-9e97-9cad0e832a61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    897664  ultralytics.nn.modules.head.Detect           [80, [64, 128, 256]]          \n",
      "YOLOv8n summary: 225 layers, 3157200 parameters, 3157184 gradients, 8.9 GFLOPs\n",
      "\n",
      "New https://pypi.org/project/ultralytics/8.0.227 available ğŸ˜ƒ Update with 'pip install -U ultralytics'\n",
      "Ultralytics YOLOv8.0.222 ğŸš€ Python-3.10.12 torch-2.1.0+cu118 CUDA:0 (Tesla T4, 14931MiB)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8n.pt, data=coco128.yaml, epochs=3, patience=50, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train37, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train37\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    897664  ultralytics.nn.modules.head.Detect           [80, [64, 128, 256]]          \n",
      "Model summary: 225 layers, 3157200 parameters, 3157184 gradients, 8.9 GFLOPs\n",
      "\n",
      "Transferred 355/355 items from pretrained weights\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/rahman2/CMPE691_CV/Project/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrounds, 0\u001b[0m\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/rahman2/CMPE691_CV/Project/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrounds, 0 c\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs/detect/train37/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000119, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1mruns/detect/train37\u001b[0m\n",
      "Starting training for 3 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        1/3      2.55G      1.226      1.615      1.274        178        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:02<00:00,  3.14i\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:01<00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        128        929      0.616      0.555      0.612      0.454\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        2/3      2.54G      1.225      1.513      1.268        231        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:02<00:00,  3.21i\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:01<00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        128        929      0.673      0.543      0.625      0.466\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        3/3      2.72G      1.209      1.449      1.222        178        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:02<00:00,  3.86i\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:02<00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        128        929      0.668      0.544       0.63      0.468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3 epochs completed in 0.004 hours.\n",
      "Optimizer stripped from runs/detect/train37/weights/last.pt, 6.5MB\n",
      "Optimizer stripped from runs/detect/train37/weights/best.pt, 6.5MB\n",
      "\n",
      "Validating runs/detect/train37/weights/best.pt...\n",
      "Ultralytics YOLOv8.0.222 ğŸš€ Python-3.10.12 torch-2.1.0+cu118 CUDA:0 (Tesla T4, 14931MiB)\n",
      "Model summary (fused): 168 layers, 3151904 parameters, 0 gradients, 8.7 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:03<00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        128        929       0.67      0.544      0.631      0.469\n",
      "                person        128        254      0.794      0.668      0.769      0.542\n",
      "               bicycle        128          6      0.578      0.333      0.328      0.281\n",
      "                   car        128         46      0.868      0.217      0.286      0.178\n",
      "            motorcycle        128          5      0.688      0.889      0.898      0.697\n",
      "              airplane        128          6      0.827        0.8      0.903      0.673\n",
      "                   bus        128          7      0.689      0.714      0.737      0.649\n",
      "                 train        128          3      0.554      0.667       0.83      0.731\n",
      "                 truck        128         12          1      0.365      0.479      0.294\n",
      "                  boat        128          6      0.292      0.167      0.351      0.221\n",
      "         traffic light        128         14      0.692      0.165      0.202      0.139\n",
      "             stop sign        128          2      0.967          1      0.995      0.711\n",
      "                 bench        128          9      0.838      0.576      0.637        0.4\n",
      "                  bird        128         16      0.857       0.75      0.879      0.525\n",
      "                   cat        128          4      0.907          1      0.995      0.818\n",
      "                   dog        128          9      0.643      0.801      0.839      0.627\n",
      "                 horse        128          2      0.532          1      0.995      0.511\n",
      "              elephant        128         17      0.853      0.765       0.88       0.67\n",
      "                  bear        128          1       0.63          1      0.995      0.995\n",
      "                 zebra        128          4      0.856          1      0.995      0.965\n",
      "               giraffe        128          9      0.805      0.889      0.963      0.721\n",
      "              backpack        128          6      0.619      0.333      0.391      0.235\n",
      "              umbrella        128         18      0.705        0.5      0.666      0.457\n",
      "               handbag        128         19      0.529     0.0619      0.189     0.0985\n",
      "                   tie        128          7      0.688      0.634       0.64      0.457\n",
      "              suitcase        128          4      0.638          1      0.828      0.596\n",
      "               frisbee        128          5       0.57        0.8      0.759      0.663\n",
      "                  skis        128          1      0.482          1      0.995      0.497\n",
      "             snowboard        128          7       0.66      0.714      0.758      0.488\n",
      "           sports ball        128          6      0.704      0.408      0.503       0.29\n",
      "                  kite        128         10      0.815        0.5      0.584      0.203\n",
      "          baseball bat        128          4      0.608       0.25      0.347      0.174\n",
      "        baseball glove        128          7      0.678      0.429      0.429      0.295\n",
      "            skateboard        128          5      0.777        0.6        0.6       0.44\n",
      "         tennis racket        128          7       0.62      0.571      0.501      0.387\n",
      "                bottle        128         18      0.534      0.382      0.357      0.219\n",
      "            wine glass        128         16      0.583      0.562       0.57      0.345\n",
      "                   cup        128         36      0.598      0.289      0.413      0.288\n",
      "                  fork        128          6      0.594      0.167      0.226      0.198\n",
      "                 knife        128         16      0.623        0.5      0.588      0.341\n",
      "                 spoon        128         22      0.642      0.182      0.331      0.184\n",
      "                  bowl        128         28      0.734      0.643      0.653      0.498\n",
      "                banana        128          1          0          0      0.166     0.0501\n",
      "              sandwich        128          2          0          0      0.497      0.497\n",
      "                orange        128          4          1      0.269      0.995      0.662\n",
      "              broccoli        128         11       0.51      0.182      0.254      0.219\n",
      "                carrot        128         24      0.748      0.494      0.656      0.416\n",
      "               hot dog        128          2      0.646      0.937      0.828      0.796\n",
      "                 pizza        128          5      0.692          1      0.995      0.866\n",
      "                 donut        128         14      0.655          1      0.929      0.848\n",
      "                  cake        128          4      0.656          1      0.995       0.88\n",
      "                 chair        128         35      0.506      0.514       0.46      0.261\n",
      "                 couch        128          6       0.78      0.599      0.719      0.543\n",
      "          potted plant        128         14      0.802      0.643      0.727      0.496\n",
      "                   bed        128          3      0.879      0.667       0.83      0.621\n",
      "          dining table        128         13      0.448      0.615      0.522       0.41\n",
      "                toilet        128          2          1      0.869      0.995      0.946\n",
      "                    tv        128          2      0.356        0.5      0.695      0.656\n",
      "                laptop        128          3          1          0      0.696        0.6\n",
      "                 mouse        128          2          1          0      0.057     0.0057\n",
      "                remote        128          8      0.846        0.5      0.587       0.51\n",
      "            cell phone        128          8          0          0     0.0609      0.041\n",
      "             microwave        128          3      0.632      0.667      0.863      0.719\n",
      "                  oven        128          5      0.443        0.4      0.338      0.269\n",
      "                  sink        128          6      0.365      0.167      0.199      0.136\n",
      "          refrigerator        128          5      0.686        0.4      0.647      0.505\n",
      "                  book        128         29      0.644      0.126      0.372      0.183\n",
      "                 clock        128          9      0.779      0.783      0.891      0.712\n",
      "                  vase        128          2      0.364          1      0.828      0.745\n",
      "              scissors        128          1          1          0      0.249     0.0746\n",
      "            teddy bear        128         21          1      0.374      0.651      0.418\n",
      "            toothbrush        128          5      0.887        0.6      0.793      0.527\n",
      "Speed: 0.3ms preprocess, 3.2ms inference, 0.0ms loss, 1.7ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/train37\u001b[0m\n",
      "Ultralytics YOLOv8.0.222 ğŸš€ Python-3.10.12 torch-2.1.0+cu118 CUDA:0 (Tesla T4, 14931MiB)\n",
      "Model summary (fused): 168 layers, 3151904 parameters, 0 gradients, 8.7 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/rahman2/CMPE691_CV/Project/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrounds, 0 c\u001b[0m\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:03<00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        128        929      0.656      0.545      0.629      0.469\n",
      "                person        128        254      0.812      0.678      0.774      0.541\n",
      "               bicycle        128          6      0.569      0.333      0.327      0.283\n",
      "                   car        128         46      0.817      0.217      0.285      0.178\n",
      "            motorcycle        128          5       0.69      0.896      0.898      0.697\n",
      "              airplane        128          6      0.828      0.805      0.903      0.681\n",
      "                   bus        128          7      0.683      0.714      0.736      0.648\n",
      "                 train        128          3       0.55      0.667       0.83      0.731\n",
      "                 truck        128         12          1      0.377      0.497      0.297\n",
      "                  boat        128          6      0.262      0.167      0.319      0.155\n",
      "         traffic light        128         14        0.7      0.171      0.202      0.139\n",
      "             stop sign        128          2      0.935          1      0.995      0.713\n",
      "                 bench        128          9      0.841       0.59      0.637        0.4\n",
      "                  bird        128         16      0.919       0.75      0.876      0.509\n",
      "                   cat        128          4      0.864          1      0.995      0.835\n",
      "                   dog        128          9      0.666      0.778      0.821      0.627\n",
      "                 horse        128          2      0.526          1      0.995      0.511\n",
      "              elephant        128         17      0.851      0.765       0.88      0.667\n",
      "                  bear        128          1      0.626          1      0.995      0.995\n",
      "                 zebra        128          4      0.854          1      0.995      0.965\n",
      "               giraffe        128          9      0.737      0.938      0.943      0.732\n",
      "              backpack        128          6       0.65      0.333      0.449      0.272\n",
      "              umbrella        128         18      0.672        0.5      0.666      0.461\n",
      "               handbag        128         19      0.447     0.0471      0.175     0.0944\n",
      "                   tie        128          7      0.693      0.651      0.641      0.457\n",
      "              suitcase        128          4      0.631          1      0.828      0.592\n",
      "               frisbee        128          5      0.565        0.8      0.759      0.663\n",
      "                  skis        128          1      0.469          1      0.995      0.497\n",
      "             snowboard        128          7      0.656      0.714      0.759      0.488\n",
      "           sports ball        128          6      0.679      0.358      0.545      0.299\n",
      "                  kite        128         10      0.806        0.5      0.595      0.208\n",
      "          baseball bat        128          4      0.588       0.25      0.347      0.198\n",
      "        baseball glove        128          7      0.671      0.429      0.429      0.316\n",
      "            skateboard        128          5      0.875        0.6        0.6       0.44\n",
      "         tennis racket        128          7      0.746      0.422      0.528      0.365\n",
      "                bottle        128         18      0.515      0.389      0.379       0.22\n",
      "            wine glass        128         16      0.576      0.562      0.569       0.35\n",
      "                   cup        128         36      0.566      0.278      0.402      0.282\n",
      "                  fork        128          6      0.588      0.167      0.228      0.196\n",
      "                 knife        128         16      0.567        0.5      0.588       0.36\n",
      "                 spoon        128         22      0.634      0.182      0.331      0.184\n",
      "                  bowl        128         28      0.738      0.643      0.658      0.498\n",
      "                banana        128          1          0          0      0.142     0.0441\n",
      "              sandwich        128          2      0.171      0.256      0.497      0.497\n",
      "                orange        128          4          1      0.321      0.995      0.666\n",
      "              broccoli        128         11      0.502      0.182      0.254      0.205\n",
      "                carrot        128         24      0.725      0.549      0.653      0.418\n",
      "               hot dog        128          2      0.648      0.944      0.828      0.796\n",
      "                 pizza        128          5      0.713          1      0.995      0.843\n",
      "                 donut        128         14      0.637          1       0.94      0.849\n",
      "                  cake        128          4      0.642          1      0.995       0.88\n",
      "                 chair        128         35      0.484      0.543      0.467       0.26\n",
      "                 couch        128          6      0.521        0.5      0.693      0.562\n",
      "          potted plant        128         14      0.635      0.571      0.707      0.491\n",
      "                   bed        128          3       0.75      0.667      0.863      0.658\n",
      "          dining table        128         13      0.455      0.615      0.516      0.407\n",
      "                toilet        128          2          1      0.874      0.995      0.946\n",
      "                    tv        128          2      0.378        0.5      0.695      0.656\n",
      "                laptop        128          3          1          0      0.605      0.484\n",
      "                 mouse        128          2          1          0     0.0625    0.00625\n",
      "                remote        128          8       0.83        0.5      0.605      0.514\n",
      "            cell phone        128          8          0          0     0.0691     0.0467\n",
      "             microwave        128          3      0.621      0.667      0.863      0.753\n",
      "                  oven        128          5      0.432        0.4      0.339       0.27\n",
      "                  sink        128          6      0.378      0.167       0.18      0.131\n",
      "          refrigerator        128          5      0.681        0.4      0.617       0.49\n",
      "                  book        128         29      0.635      0.122       0.37      0.204\n",
      "                 clock        128          9       0.78      0.788      0.894      0.734\n",
      "                  vase        128          2      0.414          1      0.828      0.745\n",
      "              scissors        128          1          1          0      0.249     0.0746\n",
      "            teddy bear        128         21      0.871      0.381      0.634      0.421\n",
      "            toothbrush        128          5      0.633        0.6      0.736      0.468\n",
      "Speed: 0.2ms preprocess, 8.0ms inference, 0.0ms loss, 3.8ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/train372\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "## Extract bounding boxes using YOLOv8\n",
    "from ultralytics import YOLO\n",
    "# Load a model\n",
    "model = YOLO(\"yolov8n.yaml\")  # build a new model from scratch\n",
    "model = YOLO(\"yolov8n.pt\")  # load a pretrained model (recommended for training)\n",
    "\n",
    "# Use the model\n",
    "model.train(data=\"coco128.yaml\", epochs=3)  # train the model\n",
    "metrics = model.val()  # evaluate model performance on the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b7528d-c516-47f5-a37d-5a6aa2d34ae1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6034a6d-cd55-4b68-b068-6ef3bbf2b336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-07-24-11-22-45+0800__CAM_FRONT__1532402927612460.jpg: 384x640 1 person, 4 cars, 1 truck, 50.9ms\n",
      "Speed: 4.5ms preprocess, 50.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-07-24-11-22-45+0800__CAM_FRONT__1532402928112460.jpg: 384x640 1 person, 7 cars, 1 truck, 5.0ms\n",
      "Speed: 1.4ms preprocess, 5.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-07-24-11-22-45+0800__CAM_FRONT__1532402928662460.jpg: 384x640 2 persons, 9 cars, 1 truck, 5.5ms\n",
      "Speed: 1.6ms preprocess, 5.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-07-24-11-22-45+0800__CAM_FRONT__1532402929162460.jpg: 384x640 6 cars, 1 truck, 5.0ms\n",
      "Speed: 1.4ms preprocess, 5.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-07-24-11-22-45+0800__CAM_FRONT__1532402929662460.jpg: 384x640 3 cars, 7.2ms\n",
      "Speed: 4.0ms preprocess, 7.2ms inference, 24.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-07-24-11-22-45+0800__CAM_FRONT__1532402930112460.jpg: 384x640 8 cars, 5.5ms\n",
      "Speed: 1.8ms preprocess, 5.5ms inference, 3.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-07-24-11-22-45+0800__CAM_FRONT__1532402930612460.jpg: 384x640 6 cars, 2 buss, 1 truck, 6.9ms\n",
      "Speed: 1.4ms preprocess, 6.9ms inference, 30.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-07-24-11-22-45+0800__CAM_FRONT__1532402931162460.jpg: 384x640 5 cars, 1 truck, 7.4ms\n",
      "Speed: 5.5ms preprocess, 7.4ms inference, 4.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-07-24-11-22-45+0800__CAM_FRONT__1532402931662460.jpg: 384x640 3 persons, 6 cars, 5.0ms\n",
      "Speed: 1.4ms preprocess, 5.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-07-24-11-22-45+0800__CAM_FRONT__1532402932162460.jpg: 384x640 3 persons, 7 cars, 7.1ms\n",
      "Speed: 4.1ms preprocess, 7.1ms inference, 25.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-07-24-11-22-45+0800__CAM_FRONT__1532402932612460.jpg: 384x640 4 persons, 6 cars, 1 traffic light, 6.0ms\n",
      "Speed: 1.4ms preprocess, 6.0ms inference, 17.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-07-24-11-22-45+0800__CAM_FRONT__1532402933112460.jpg: 384x640 5 persons, 5 cars, 2 trucks, 7.1ms\n",
      "Speed: 4.6ms preprocess, 7.1ms inference, 29.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-07-24-11-22-45+0800__CAM_FRONT__1532402933612460.jpg: 384x640 4 persons, 5 cars, 1 bus, 1 truck, 2 traffic lights, 7.1ms\n",
      "Speed: 3.8ms preprocess, 7.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-07-24-11-22-45+0800__CAM_FRONT__1532402934112460.jpg: 384x640 10 persons, 3 cars, 2 trucks, 2 traffic lights, 1 fire hydrant, 6.0ms\n",
      "Speed: 1.3ms preprocess, 6.0ms inference, 30.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-07-24-11-22-45+0800__CAM_FRONT__1532402934662484.jpg: 384x640 8 persons, 3 cars, 1 truck, 6.8ms\n",
      "Speed: 3.8ms preprocess, 6.8ms inference, 10.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-07-24-11-22-45+0800__CAM_FRONT__1532402935162463.jpg: 384x640 7 persons, 2 cars, 1 truck, 1 traffic light, 1 stop sign, 7.1ms\n",
      "Speed: 4.1ms preprocess, 7.1ms inference, 30.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-07-24-11-22-45+0800__CAM_FRONT__1532402935662460.jpg: 384x640 1 person, 5 cars, 1 truck, 1 fire hydrant, 5.2ms\n",
      "Speed: 1.3ms preprocess, 5.2ms inference, 3.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-07-24-11-22-45+0800__CAM_FRONT__1532402936162460.jpg: 384x640 4 cars, 2 trucks, 7.9ms\n",
      "Speed: 5.1ms preprocess, 7.9ms inference, 31.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-07-24-11-22-45+0800__CAM_FRONT__1532402936662460.jpg: 384x640 4 cars, 2 trucks, 5.1ms\n",
      "Speed: 1.7ms preprocess, 5.1ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-07-24-11-22-45+0800__CAM_FRONT__1532402937162460.jpg: 384x640 2 cars, 2 trucks, 7.3ms\n",
      "Speed: 1.4ms preprocess, 7.3ms inference, 30.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-07-24-11-22-45+0800__CAM_FRONT__1532402937662460.jpg: 384x640 2 cars, 1 bus, 1 truck, 1 traffic light, 1 fire hydrant, 7.5ms\n",
      "Speed: 3.8ms preprocess, 7.5ms inference, 8.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-07-24-11-22-45+0800__CAM_FRONT__1532402938162460.jpg: 384x640 2 cars, 3 trucks, 4.9ms\n",
      "Speed: 1.3ms preprocess, 4.9ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-07-24-11-22-45+0800__CAM_FRONT__1532402938612460.jpg: 384x640 1 car, 3 trucks, 1 fire hydrant, 5.0ms\n",
      "Speed: 2.7ms preprocess, 5.0ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-07-24-11-22-45+0800__CAM_FRONT__1532402939112460.jpg: 384x640 2 trucks, 1 fire hydrant, 5.3ms\n",
      "Speed: 1.3ms preprocess, 5.3ms inference, 3.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-07-24-11-22-45+0800__CAM_FRONT__1532402939662460.jpg: 384x640 1 car, 2 trucks, 2 fire hydrants, 5.0ms\n",
      "Speed: 1.6ms preprocess, 5.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-07-24-11-22-45+0800__CAM_FRONT__1532402940162460.jpg: 384x640 1 truck, 1 fire hydrant, 4.9ms\n",
      "Speed: 1.3ms preprocess, 4.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-07-24-11-22-45+0800__CAM_FRONT__1532402940762460.jpg: 384x640 1 truck, 4.9ms\n",
      "Speed: 1.4ms preprocess, 4.9ms inference, 3.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-07-24-11-22-45+0800__CAM_FRONT__1532402941262460.jpg: 384x640 1 truck, 1 fire hydrant, 4.8ms\n",
      "Speed: 1.3ms preprocess, 4.8ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-07-24-11-22-45+0800__CAM_FRONT__1532402941762460.jpg: 384x640 1 truck, 1 fire hydrant, 5.3ms\n",
      "Speed: 1.5ms preprocess, 5.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-07-24-11-22-45+0800__CAM_FRONT__1532402942162460.jpg: 384x640 1 car, 1 truck, 2 fire hydrants, 5.1ms\n",
      "Speed: 1.3ms preprocess, 5.1ms inference, 9.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-07-24-11-22-45+0800__CAM_FRONT__1532402942662460.jpg: 384x640 1 truck, 4.9ms\n",
      "Speed: 1.6ms preprocess, 4.9ms inference, 4.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-07-24-11-22-45+0800__CAM_FRONT__1532402943162460.jpg: 384x640 1 car, 1 truck, 4.8ms\n",
      "Speed: 1.3ms preprocess, 4.8ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-07-24-11-22-45+0800__CAM_FRONT__1532402943662460.jpg: 384x640 1 person, 1 truck, 5.8ms\n",
      "Speed: 5.3ms preprocess, 5.8ms inference, 5.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-07-24-11-22-45+0800__CAM_FRONT__1532402944162460.jpg: 384x640 1 truck, 7.5ms\n",
      "Speed: 2.6ms preprocess, 7.5ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-07-24-11-22-45+0800__CAM_FRONT__1532402944662460.jpg: 384x640 2 cars, 4.8ms\n",
      "Speed: 1.3ms preprocess, 4.8ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-07-24-11-22-45+0800__CAM_FRONT__1532402945162460.jpg: 384x640 1 car, 4.8ms\n",
      "Speed: 1.3ms preprocess, 4.8ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-07-24-11-22-45+0800__CAM_FRONT__1532402945662460.jpg: 384x640 2 cars, 5.3ms\n",
      "Speed: 1.6ms preprocess, 5.3ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-07-24-11-22-45+0800__CAM_FRONT__1532402946262460.jpg: 384x640 1 car, 1 bus, 1 fire hydrant, 4.9ms\n",
      "Speed: 1.2ms preprocess, 4.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-07-24-11-22-45+0800__CAM_FRONT__1532402946762460.jpg: 384x640 2 bicycles, 1 car, 1 truck, 4.7ms\n",
      "Speed: 1.5ms preprocess, 4.7ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n008-2018-08-01-15-16-36-0400__CAM_FRONT__1533151603512404.jpg: 384x640 6 persons, 8 cars, 3 traffic lights, 4.9ms\n",
      "Speed: 1.2ms preprocess, 4.9ms inference, 4.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n008-2018-08-01-15-16-36-0400__CAM_FRONT__1533151604012404.jpg: 384x640 8 persons, 9 cars, 2 traffic lights, 4.7ms\n",
      "Speed: 1.2ms preprocess, 4.7ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n008-2018-08-01-15-16-36-0400__CAM_FRONT__1533151604512404.jpg: 384x640 10 persons, 10 cars, 2 traffic lights, 5.0ms\n",
      "Speed: 2.1ms preprocess, 5.0ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n008-2018-08-01-15-16-36-0400__CAM_FRONT__1533151605012404.jpg: 384x640 8 persons, 12 cars, 5 traffic lights, 5.7ms\n",
      "Speed: 1.2ms preprocess, 5.7ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n008-2018-08-01-15-16-36-0400__CAM_FRONT__1533151605512404.jpg: 384x640 7 persons, 11 cars, 2 traffic lights, 4.7ms\n",
      "Speed: 3.3ms preprocess, 4.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n008-2018-08-01-15-16-36-0400__CAM_FRONT__1533151606012404.jpg: 384x640 4 persons, 12 cars, 2 traffic lights, 4.8ms\n",
      "Speed: 1.2ms preprocess, 4.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n008-2018-08-01-15-16-36-0400__CAM_FRONT__1533151606512404.jpg: 384x640 2 persons, 9 cars, 2 traffic lights, 5.5ms\n",
      "Speed: 1.3ms preprocess, 5.5ms inference, 3.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n008-2018-08-01-15-16-36-0400__CAM_FRONT__1533151607012404.jpg: 384x640 2 persons, 13 cars, 2 traffic lights, 6.2ms\n",
      "Speed: 1.4ms preprocess, 6.2ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n008-2018-08-01-15-16-36-0400__CAM_FRONT__1533151607512404.jpg: 384x640 1 person, 12 cars, 5.1ms\n",
      "Speed: 3.6ms preprocess, 5.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n008-2018-08-01-15-16-36-0400__CAM_FRONT__1533151608012404.jpg: 384x640 1 person, 14 cars, 7.5ms\n",
      "Speed: 2.5ms preprocess, 7.5ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n008-2018-08-01-15-16-36-0400__CAM_FRONT__1533151608512404.jpg: 384x640 1 person, 14 cars, 4.8ms\n",
      "Speed: 1.3ms preprocess, 4.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n008-2018-08-01-15-16-36-0400__CAM_FRONT__1533151609012404.jpg: 384x640 1 person, 17 cars, 1 traffic light, 4.7ms\n",
      "Speed: 1.3ms preprocess, 4.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n008-2018-08-01-15-16-36-0400__CAM_FRONT__1533151609512404.jpg: 384x640 14 cars, 4.7ms\n",
      "Speed: 1.3ms preprocess, 4.7ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n008-2018-08-01-15-16-36-0400__CAM_FRONT__1533151609912404.jpg: 384x640 1 person, 16 cars, 4.8ms\n",
      "Speed: 1.3ms preprocess, 4.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n008-2018-08-01-15-16-36-0400__CAM_FRONT__1533151610412404.jpg: 384x640 19 cars, 7.1ms\n",
      "Speed: 5.5ms preprocess, 7.1ms inference, 9.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n008-2018-08-01-15-16-36-0400__CAM_FRONT__1533151610912404.jpg: 384x640 2 persons, 14 cars, 5.6ms\n",
      "Speed: 1.3ms preprocess, 5.6ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n008-2018-08-01-15-16-36-0400__CAM_FRONT__1533151611412404.jpg: 384x640 1 person, 10 cars, 5.7ms\n",
      "Speed: 1.2ms preprocess, 5.7ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n008-2018-08-01-15-16-36-0400__CAM_FRONT__1533151611862404.jpg: 384x640 2 persons, 12 cars, 4.9ms\n",
      "Speed: 1.7ms preprocess, 4.9ms inference, 7.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n008-2018-08-01-15-16-36-0400__CAM_FRONT__1533151612362404.jpg: 384x640 10 cars, 4.7ms\n",
      "Speed: 2.1ms preprocess, 4.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n008-2018-08-01-15-16-36-0400__CAM_FRONT__1533151612862404.jpg: 384x640 13 cars, 4.8ms\n",
      "Speed: 1.3ms preprocess, 4.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n008-2018-08-01-15-16-36-0400__CAM_FRONT__1533151613362404.jpg: 384x640 10 cars, 5.7ms\n",
      "Speed: 1.3ms preprocess, 5.7ms inference, 7.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n008-2018-08-01-15-16-36-0400__CAM_FRONT__1533151613912404.jpg: 384x640 15 cars, 6.8ms\n",
      "Speed: 3.5ms preprocess, 6.8ms inference, 8.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n008-2018-08-01-15-16-36-0400__CAM_FRONT__1533151614412404.jpg: 384x640 16 cars, 5.5ms\n",
      "Speed: 2.5ms preprocess, 5.5ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n008-2018-08-01-15-16-36-0400__CAM_FRONT__1533151614912404.jpg: 384x640 1 person, 20 cars, 4.8ms\n",
      "Speed: 1.3ms preprocess, 4.8ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n008-2018-08-01-15-16-36-0400__CAM_FRONT__1533151615412404.jpg: 384x640 1 person, 20 cars, 4.9ms\n",
      "Speed: 1.4ms preprocess, 4.9ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n008-2018-08-01-15-16-36-0400__CAM_FRONT__1533151615912404.jpg: 384x640 1 person, 3 bicycles, 15 cars, 4.9ms\n",
      "Speed: 1.3ms preprocess, 4.9ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n008-2018-08-01-15-16-36-0400__CAM_FRONT__1533151616412404.jpg: 384x640 1 person, 17 cars, 4.8ms\n",
      "Speed: 1.3ms preprocess, 4.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n008-2018-08-01-15-16-36-0400__CAM_FRONT__1533151616912404.jpg: 384x640 2 persons, 8 bicycles, 23 cars, 1 truck, 7.0ms\n",
      "Speed: 4.2ms preprocess, 7.0ms inference, 11.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n008-2018-08-01-15-16-36-0400__CAM_FRONT__1533151617362404.jpg: 384x640 1 person, 4 bicycles, 25 cars, 1 truck, 6.9ms\n",
      "Speed: 1.8ms preprocess, 6.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n008-2018-08-01-15-16-36-0400__CAM_FRONT__1533151617912404.jpg: 384x640 1 person, 21 cars, 1 truck, 4.8ms\n",
      "Speed: 1.3ms preprocess, 4.8ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n008-2018-08-01-15-16-36-0400__CAM_FRONT__1533151618412404.jpg: 384x640 4 bicycles, 19 cars, 5.9ms\n",
      "Speed: 1.3ms preprocess, 5.9ms inference, 11.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n008-2018-08-01-15-16-36-0400__CAM_FRONT__1533151618912404.jpg: 384x640 1 person, 3 bicycles, 21 cars, 1 bus, 5.1ms\n",
      "Speed: 1.2ms preprocess, 5.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n008-2018-08-01-15-16-36-0400__CAM_FRONT__1533151619412404.jpg: 384x640 3 persons, 1 bicycle, 21 cars, 1 truck, 4.8ms\n",
      "Speed: 1.2ms preprocess, 4.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n008-2018-08-01-15-16-36-0400__CAM_FRONT__1533151619912458.jpg: 384x640 1 person, 22 cars, 1 truck, 5.3ms\n",
      "Speed: 4.3ms preprocess, 5.3ms inference, 5.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n008-2018-08-01-15-16-36-0400__CAM_FRONT__1533151620412404.jpg: 384x640 16 cars, 1 truck, 1 traffic light, 5.3ms\n",
      "Speed: 3.0ms preprocess, 5.3ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n008-2018-08-01-15-16-36-0400__CAM_FRONT__1533151621012404.jpg: 384x640 21 cars, 1 truck, 5.1ms\n",
      "Speed: 1.7ms preprocess, 5.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n008-2018-08-01-15-16-36-0400__CAM_FRONT__1533151621412404.jpg: 384x640 18 cars, 4.8ms\n",
      "Speed: 1.6ms preprocess, 4.8ms inference, 3.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n008-2018-08-01-15-16-36-0400__CAM_FRONT__1533151621912404.jpg: 384x640 25 cars, 4.7ms\n",
      "Speed: 1.3ms preprocess, 4.7ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n008-2018-08-01-15-16-36-0400__CAM_FRONT__1533151622412404.jpg: 384x640 1 bicycle, 22 cars, 4.6ms\n",
      "Speed: 1.3ms preprocess, 4.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n008-2018-08-01-15-16-36-0400__CAM_FRONT__1533151622912404.jpg: 384x640 27 cars, 7.0ms\n",
      "Speed: 1.3ms preprocess, 7.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n008-2018-08-28-16-43-51-0400__CAM_FRONT__1535489296012404.jpg: 384x640 1 person, 1 bicycle, 6 traffic lights, 4.7ms\n",
      "Speed: 1.2ms preprocess, 4.7ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n008-2018-08-28-16-43-51-0400__CAM_FRONT__1535489296512409.jpg: 384x640 1 person, 2 bicycles, 6 traffic lights, 4.6ms\n",
      "Speed: 1.2ms preprocess, 4.6ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n008-2018-08-28-16-43-51-0400__CAM_FRONT__1535489297012404.jpg: 384x640 1 car, 6 traffic lights, 4.7ms\n",
      "Speed: 1.4ms preprocess, 4.7ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n008-2018-08-28-16-43-51-0400__CAM_FRONT__1535489297512404.jpg: 384x640 1 car, 6 traffic lights, 4.9ms\n",
      "Speed: 1.5ms preprocess, 4.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n008-2018-08-28-16-43-51-0400__CAM_FRONT__1535489298012404.jpg: 384x640 1 car, 6 traffic lights, 4.9ms\n",
      "Speed: 1.4ms preprocess, 4.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n008-2018-08-28-16-43-51-0400__CAM_FRONT__1535489298512404.jpg: 384x640 2 cars, 6 traffic lights, 4.7ms\n",
      "Speed: 1.2ms preprocess, 4.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n008-2018-08-28-16-43-51-0400__CAM_FRONT__1535489299012404.jpg: 384x640 2 cars, 5 traffic lights, 4.6ms\n",
      "Speed: 1.3ms preprocess, 4.6ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n008-2018-08-28-16-43-51-0400__CAM_FRONT__1535489299512404.jpg: 384x640 2 cars, 6 traffic lights, 4.6ms\n",
      "Speed: 1.2ms preprocess, 4.6ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n008-2018-08-28-16-43-51-0400__CAM_FRONT__1535489300012404.jpg: 384x640 3 cars, 6 traffic lights, 4.6ms\n",
      "Speed: 1.2ms preprocess, 4.6ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n008-2018-08-28-16-43-51-0400__CAM_FRONT__1535489300512404.jpg: 384x640 2 cars, 2 trucks, 6 traffic lights, 4.6ms\n",
      "Speed: 1.2ms preprocess, 4.6ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n008-2018-08-28-16-43-51-0400__CAM_FRONT__1535489301012404.jpg: 384x640 2 cars, 2 trucks, 4 traffic lights, 4.7ms\n",
      "Speed: 1.3ms preprocess, 4.7ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n008-2018-08-28-16-43-51-0400__CAM_FRONT__1535489301512404.jpg: 384x640 1 car, 1 bus, 1 truck, 5 traffic lights, 4.6ms\n",
      "Speed: 1.2ms preprocess, 4.6ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n008-2018-08-28-16-43-51-0400__CAM_FRONT__1535489302012404.jpg: 384x640 2 cars, 1 bus, 1 truck, 7 traffic lights, 4.7ms\n",
      "Speed: 1.2ms preprocess, 4.7ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n008-2018-08-28-16-43-51-0400__CAM_FRONT__1535489302512404.jpg: 384x640 2 cars, 2 buss, 6 traffic lights, 4.8ms\n",
      "Speed: 1.2ms preprocess, 4.8ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n008-2018-08-28-16-43-51-0400__CAM_FRONT__1535489303012421.jpg: 384x640 1 person, 1 car, 1 bus, 1 train, 6 traffic lights, 4.7ms\n",
      "Speed: 1.3ms preprocess, 4.7ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n008-2018-08-28-16-43-51-0400__CAM_FRONT__1535489303412404.jpg: 384x640 3 persons, 2 cars, 1 bus, 6 traffic lights, 4.7ms\n",
      "Speed: 1.3ms preprocess, 4.7ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n008-2018-08-28-16-43-51-0400__CAM_FRONT__1535489303912404.jpg: 384x640 2 persons, 2 cars, 1 bus, 6 traffic lights, 4.7ms\n",
      "Speed: 1.2ms preprocess, 4.7ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n008-2018-08-28-16-43-51-0400__CAM_FRONT__1535489304412404.jpg: 384x640 2 persons, 2 cars, 6 traffic lights, 2 handbags, 5.0ms\n",
      "Speed: 1.3ms preprocess, 5.0ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n008-2018-08-28-16-43-51-0400__CAM_FRONT__1535489304912404.jpg: 384x640 2 persons, 1 car, 5 traffic lights, 1 handbag, 5.0ms\n",
      "Speed: 1.3ms preprocess, 5.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n008-2018-08-28-16-43-51-0400__CAM_FRONT__1535489305412404.jpg: 384x640 4 persons, 2 cars, 6 traffic lights, 4.9ms\n",
      "Speed: 1.3ms preprocess, 4.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n008-2018-08-28-16-43-51-0400__CAM_FRONT__1535489305912404.jpg: 384x640 5 persons, 3 cars, 6 traffic lights, 1 backpack, 4.9ms\n",
      "Speed: 1.3ms preprocess, 4.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n008-2018-08-28-16-43-51-0400__CAM_FRONT__1535489306412404.jpg: 384x640 3 persons, 4 cars, 1 truck, 6 traffic lights, 4.9ms\n",
      "Speed: 1.3ms preprocess, 4.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n008-2018-08-28-16-43-51-0400__CAM_FRONT__1535489306912404.jpg: 384x640 5 persons, 2 cars, 6 traffic lights, 4.9ms\n",
      "Speed: 1.2ms preprocess, 4.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n008-2018-08-28-16-43-51-0400__CAM_FRONT__1535489307412404.jpg: 384x640 4 persons, 2 cars, 1 truck, 5 traffic lights, 1 backpack, 4.8ms\n",
      "Speed: 1.2ms preprocess, 4.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n008-2018-08-28-16-43-51-0400__CAM_FRONT__1535489307912404.jpg: 384x640 5 persons, 3 cars, 1 bus, 1 truck, 5 traffic lights, 4.7ms\n",
      "Speed: 1.2ms preprocess, 4.7ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n008-2018-08-28-16-43-51-0400__CAM_FRONT__1535489308362404.jpg: 384x640 5 persons, 5 cars, 5 traffic lights, 4.7ms\n",
      "Speed: 1.2ms preprocess, 4.7ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n008-2018-08-28-16-43-51-0400__CAM_FRONT__1535489308862404.jpg: 384x640 6 persons, 4 cars, 6 traffic lights, 1 skateboard, 4.7ms\n",
      "Speed: 1.3ms preprocess, 4.7ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n008-2018-08-28-16-43-51-0400__CAM_FRONT__1535489309362404.jpg: 384x640 6 persons, 1 car, 6 traffic lights, 4.7ms\n",
      "Speed: 1.9ms preprocess, 4.7ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n008-2018-08-28-16-43-51-0400__CAM_FRONT__1535489309862404.jpg: 384x640 5 persons, 3 cars, 6 traffic lights, 2 skateboards, 4.7ms\n",
      "Speed: 1.2ms preprocess, 4.7ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n008-2018-08-28-16-43-51-0400__CAM_FRONT__1535489310362404.jpg: 384x640 6 persons, 4 cars, 6 traffic lights, 1 backpack, 7.0ms\n",
      "Speed: 4.6ms preprocess, 7.0ms inference, 23.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n008-2018-08-28-16-43-51-0400__CAM_FRONT__1535489310912404.jpg: 384x640 4 persons, 3 cars, 6 traffic lights, 2 backpacks, 7.2ms\n",
      "Speed: 3.2ms preprocess, 7.2ms inference, 6.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n008-2018-08-28-16-43-51-0400__CAM_FRONT__1535489311362404.jpg: 384x640 4 persons, 2 cars, 6 traffic lights, 7.2ms\n",
      "Speed: 5.6ms preprocess, 7.2ms inference, 30.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n008-2018-08-28-16-43-51-0400__CAM_FRONT__1535489311862404.jpg: 384x640 4 persons, 2 cars, 6 traffic lights, 5.0ms\n",
      "Speed: 1.3ms preprocess, 5.0ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n008-2018-08-28-16-43-51-0400__CAM_FRONT__1535489312362404.jpg: 384x640 3 persons, 2 cars, 6 traffic lights, 7.2ms\n",
      "Speed: 4.3ms preprocess, 7.2ms inference, 29.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n008-2018-08-28-16-43-51-0400__CAM_FRONT__1535489312862404.jpg: 384x640 3 persons, 2 cars, 5 traffic lights, 1 backpack, 1 skateboard, 5.2ms\n",
      "Speed: 1.4ms preprocess, 5.2ms inference, 10.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n008-2018-08-28-16-43-51-0400__CAM_FRONT__1535489313362404.jpg: 384x640 3 persons, 2 cars, 6 traffic lights, 1 backpack, 9.7ms\n",
      "Speed: 5.6ms preprocess, 9.7ms inference, 30.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n008-2018-08-28-16-43-51-0400__CAM_FRONT__1535489313862404.jpg: 384x640 3 persons, 3 cars, 6 traffic lights, 1 backpack, 9.7ms\n",
      "Speed: 2.9ms preprocess, 9.7ms inference, 30.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n008-2018-08-28-16-43-51-0400__CAM_FRONT__1535489314362404.jpg: 384x640 2 persons, 2 cars, 7 traffic lights, 8.8ms\n",
      "Speed: 5.0ms preprocess, 8.8ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n008-2018-08-28-16-43-51-0400__CAM_FRONT__1535489314862404.jpg: 384x640 2 persons, 2 cars, 6 traffic lights, 9.3ms\n",
      "Speed: 5.5ms preprocess, 9.3ms inference, 30.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n008-2018-08-28-16-43-51-0400__CAM_FRONT__1535489315412404.jpg: 384x640 1 person, 2 cars, 6 traffic lights, 10.0ms\n",
      "Speed: 2.4ms preprocess, 10.0ms inference, 29.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n008-2018-08-28-16-43-51-0400__CAM_FRONT__1535489315912404.jpg: 384x640 1 person, 2 cars, 6 traffic lights, 10.7ms\n",
      "Speed: 4.9ms preprocess, 10.7ms inference, 6.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n008-2018-08-27-11-48-51-0400__CAM_FRONT__1535385092112404.jpg: 384x640 14 cars, 1 truck, 11.6ms\n",
      "Speed: 4.4ms preprocess, 11.6ms inference, 30.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n008-2018-08-27-11-48-51-0400__CAM_FRONT__1535385092662404.jpg: 384x640 19 cars, 1 truck, 8.2ms\n",
      "Speed: 2.1ms preprocess, 8.2ms inference, 21.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n008-2018-08-27-11-48-51-0400__CAM_FRONT__1535385093162404.jpg: 384x640 19 cars, 2 trucks, 10.7ms\n",
      "Speed: 2.4ms preprocess, 10.7ms inference, 13.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n008-2018-08-27-11-48-51-0400__CAM_FRONT__1535385093612404.jpg: 384x640 13 cars, 2 trucks, 9.4ms\n",
      "Speed: 4.9ms preprocess, 9.4ms inference, 30.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n008-2018-08-27-11-48-51-0400__CAM_FRONT__1535385094112404.jpg: 384x640 12 cars, 7.8ms\n",
      "Speed: 2.0ms preprocess, 7.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n008-2018-08-27-11-48-51-0400__CAM_FRONT__1535385094512404.jpg: 384x640 12 cars, 9.5ms\n",
      "Speed: 5.1ms preprocess, 9.5ms inference, 4.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n008-2018-08-27-11-48-51-0400__CAM_FRONT__1535385094912404.jpg: 384x640 14 cars, 8.0ms\n",
      "Speed: 5.2ms preprocess, 8.0ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n008-2018-08-27-11-48-51-0400__CAM_FRONT__1535385095412404.jpg: 384x640 16 cars, 1 truck, 9.5ms\n",
      "Speed: 5.8ms preprocess, 9.5ms inference, 30.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n008-2018-08-27-11-48-51-0400__CAM_FRONT__1535385095912404.jpg: 384x640 1 person, 16 cars, 1 truck, 7.8ms\n",
      "Speed: 2.0ms preprocess, 7.8ms inference, 30.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n008-2018-08-27-11-48-51-0400__CAM_FRONT__1535385096362404.jpg: 384x640 12 cars, 8.1ms\n",
      "Speed: 6.2ms preprocess, 8.1ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n008-2018-08-27-11-48-51-0400__CAM_FRONT__1535385096862404.jpg: 384x640 16 cars, 9.4ms\n",
      "Speed: 5.5ms preprocess, 9.4ms inference, 30.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n008-2018-08-27-11-48-51-0400__CAM_FRONT__1535385097362404.jpg: 384x640 12 cars, 9.7ms\n",
      "Speed: 2.0ms preprocess, 9.7ms inference, 40.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n008-2018-08-27-11-48-51-0400__CAM_FRONT__1535385097862411.jpg: 384x640 1 person, 13 cars, 1 truck, 22.4ms\n",
      "Speed: 14.2ms preprocess, 22.4ms inference, 30.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n008-2018-08-27-11-48-51-0400__CAM_FRONT__1535385098362404.jpg: 384x640 2 persons, 11 cars, 7.7ms\n",
      "Speed: 2.7ms preprocess, 7.7ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n008-2018-08-27-11-48-51-0400__CAM_FRONT__1535385098862404.jpg: 384x640 10 cars, 1 truck, 7.4ms\n",
      "Speed: 2.6ms preprocess, 7.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n008-2018-08-27-11-48-51-0400__CAM_FRONT__1535385099362404.jpg: 384x640 12 cars, 7.0ms\n",
      "Speed: 6.4ms preprocess, 7.0ms inference, 25.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n008-2018-08-27-11-48-51-0400__CAM_FRONT__1535385099862404.jpg: 384x640 11 cars, 1 bus, 1 truck, 7.3ms\n",
      "Speed: 5.8ms preprocess, 7.3ms inference, 9.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n008-2018-08-27-11-48-51-0400__CAM_FRONT__1535385100362404.jpg: 384x640 1 person, 12 cars, 1 bus, 9.5ms\n",
      "Speed: 2.9ms preprocess, 9.5ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n008-2018-08-27-11-48-51-0400__CAM_FRONT__1535385100862404.jpg: 384x640 2 persons, 10 cars, 1 bus, 4.8ms\n",
      "Speed: 2.0ms preprocess, 4.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n008-2018-08-27-11-48-51-0400__CAM_FRONT__1535385101362404.jpg: 384x640 1 person, 8 cars, 1 bus, 4.9ms\n",
      "Speed: 1.3ms preprocess, 4.9ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n008-2018-08-27-11-48-51-0400__CAM_FRONT__1535385101912404.jpg: 384x640 8 cars, 1 bus, 4.9ms\n",
      "Speed: 1.3ms preprocess, 4.9ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n008-2018-08-27-11-48-51-0400__CAM_FRONT__1535385102412404.jpg: 384x640 13 cars, 1 bus, 1 truck, 6.0ms\n",
      "Speed: 1.2ms preprocess, 6.0ms inference, 24.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n008-2018-08-27-11-48-51-0400__CAM_FRONT__1535385102912404.jpg: 384x640 11 cars, 1 bus, 4.9ms\n",
      "Speed: 2.5ms preprocess, 4.9ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n008-2018-08-27-11-48-51-0400__CAM_FRONT__1535385103412404.jpg: 384x640 9 cars, 1 bus, 6.5ms\n",
      "Speed: 1.4ms preprocess, 6.5ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n008-2018-08-27-11-48-51-0400__CAM_FRONT__1535385103912404.jpg: 384x640 8 cars, 1 bus, 4.9ms\n",
      "Speed: 3.4ms preprocess, 4.9ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n008-2018-08-27-11-48-51-0400__CAM_FRONT__1535385104412404.jpg: 384x640 6 cars, 1 bus, 1 truck, 6.4ms\n",
      "Speed: 1.7ms preprocess, 6.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n008-2018-08-27-11-48-51-0400__CAM_FRONT__1535385104912404.jpg: 384x640 6 cars, 2 buss, 2 trucks, 5.3ms\n",
      "Speed: 1.3ms preprocess, 5.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n008-2018-08-27-11-48-51-0400__CAM_FRONT__1535385105412404.jpg: 384x640 1 person, 9 cars, 1 bus, 2 trucks, 4.7ms\n",
      "Speed: 1.2ms preprocess, 4.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n008-2018-08-27-11-48-51-0400__CAM_FRONT__1535385105912404.jpg: 384x640 1 car, 1 bus, 2 trucks, 4.7ms\n",
      "Speed: 1.3ms preprocess, 4.7ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n008-2018-08-27-11-48-51-0400__CAM_FRONT__1535385106412404.jpg: 384x640 1 car, 2 buss, 1 truck, 4.8ms\n",
      "Speed: 1.3ms preprocess, 4.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n008-2018-08-27-11-48-51-0400__CAM_FRONT__1535385106912404.jpg: 384x640 2 cars, 1 bus, 1 truck, 7.2ms\n",
      "Speed: 4.0ms preprocess, 7.2ms inference, 30.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n008-2018-08-27-11-48-51-0400__CAM_FRONT__1535385107412404.jpg: 384x640 2 cars, 1 bus, 1 truck, 4.8ms\n",
      "Speed: 1.4ms preprocess, 4.8ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n008-2018-08-27-11-48-51-0400__CAM_FRONT__1535385107912404.jpg: 384x640 1 car, 1 bus, 1 truck, 1 fire hydrant, 4.7ms\n",
      "Speed: 2.0ms preprocess, 4.7ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n008-2018-08-27-11-48-51-0400__CAM_FRONT__1535385108412412.jpg: 384x640 4 cars, 6.5ms\n",
      "Speed: 1.7ms preprocess, 6.5ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n008-2018-08-27-11-48-51-0400__CAM_FRONT__1535385108912404.jpg: 384x640 2 cars, 1 fire hydrant, 4.7ms\n",
      "Speed: 1.6ms preprocess, 4.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n008-2018-08-27-11-48-51-0400__CAM_FRONT__1535385109362404.jpg: 384x640 1 car, 4.8ms\n",
      "Speed: 1.2ms preprocess, 4.8ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n008-2018-08-27-11-48-51-0400__CAM_FRONT__1535385109862404.jpg: 384x640 2 cars, 4.8ms\n",
      "Speed: 1.2ms preprocess, 4.8ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n008-2018-08-27-11-48-51-0400__CAM_FRONT__1535385110412404.jpg: 384x640 2 cars, 1 truck, 4.9ms\n",
      "Speed: 1.3ms preprocess, 4.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n008-2018-08-27-11-48-51-0400__CAM_FRONT__1535385110912404.jpg: 384x640 1 car, 5.2ms\n",
      "Speed: 1.3ms preprocess, 5.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n008-2018-08-27-11-48-51-0400__CAM_FRONT__1535385111412404.jpg: 384x640 3 cars, 7.1ms\n",
      "Speed: 4.9ms preprocess, 7.1ms inference, 30.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n008-2018-08-27-11-48-51-0400__CAM_FRONT__1535385111912404.jpg: 384x640 1 car, 4.9ms\n",
      "Speed: 3.5ms preprocess, 4.9ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n008-2018-08-30-15-16-55-0400__CAM_FRONT__1535657108262404.jpg: 384x640 1 car, 7.4ms\n",
      "Speed: 3.6ms preprocess, 7.4ms inference, 14.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n008-2018-08-30-15-16-55-0400__CAM_FRONT__1535657108762404.jpg: 384x640 1 traffic light, 8.4ms\n",
      "Speed: 2.4ms preprocess, 8.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n008-2018-08-30-15-16-55-0400__CAM_FRONT__1535657109262404.jpg: 384x640 3 cars, 7.9ms\n",
      "Speed: 2.0ms preprocess, 7.9ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n008-2018-08-30-15-16-55-0400__CAM_FRONT__1535657109762404.jpg: 384x640 2 cars, 2 traffic lights, 7.8ms\n",
      "Speed: 1.9ms preprocess, 7.8ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n008-2018-08-30-15-16-55-0400__CAM_FRONT__1535657110162404.jpg: 384x640 2 cars, 2 traffic lights, 7.8ms\n",
      "Speed: 1.9ms preprocess, 7.8ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n008-2018-08-30-15-16-55-0400__CAM_FRONT__1535657110612404.jpg: 384x640 1 car, 1 train, 3 traffic lights, 7.8ms\n",
      "Speed: 1.9ms preprocess, 7.8ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n008-2018-08-30-15-16-55-0400__CAM_FRONT__1535657111112404.jpg: 384x640 1 car, 2 buss, 2 traffic lights, 8.2ms\n",
      "Speed: 1.9ms preprocess, 8.2ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n008-2018-08-30-15-16-55-0400__CAM_FRONT__1535657111612404.jpg: 384x640 3 cars, 1 bus, 1 traffic light, 7.7ms\n",
      "Speed: 1.9ms preprocess, 7.7ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n008-2018-08-30-15-16-55-0400__CAM_FRONT__1535657112112404.jpg: 384x640 3 cars, 1 bus, 1 traffic light, 7.7ms\n",
      "Speed: 1.9ms preprocess, 7.7ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n008-2018-08-30-15-16-55-0400__CAM_FRONT__1535657112612404.jpg: 384x640 3 cars, 1 bus, 1 traffic light, 7.7ms\n",
      "Speed: 1.9ms preprocess, 7.7ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n008-2018-08-30-15-16-55-0400__CAM_FRONT__1535657113112404.jpg: 384x640 1 person, 2 cars, 1 bus, 1 truck, 2 traffic lights, 1 fire hydrant, 7.7ms\n",
      "Speed: 1.9ms preprocess, 7.7ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n008-2018-08-30-15-16-55-0400__CAM_FRONT__1535657113612404.jpg: 384x640 1 car, 1 bus, 1 fire hydrant, 7.8ms\n",
      "Speed: 1.9ms preprocess, 7.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n008-2018-08-30-15-16-55-0400__CAM_FRONT__1535657114112404.jpg: 384x640 1 person, 1 car, 1 bus, 1 traffic light, 8.0ms\n",
      "Speed: 1.9ms preprocess, 8.0ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n008-2018-08-30-15-16-55-0400__CAM_FRONT__1535657114612404.jpg: 384x640 2 cars, 1 bus, 1 truck, 1 traffic light, 7.8ms\n",
      "Speed: 1.9ms preprocess, 7.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n008-2018-08-30-15-16-55-0400__CAM_FRONT__1535657115112404.jpg: 384x640 2 cars, 1 bus, 3 traffic lights, 7.9ms\n",
      "Speed: 1.9ms preprocess, 7.9ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n008-2018-08-30-15-16-55-0400__CAM_FRONT__1535657115612404.jpg: 384x640 2 cars, 1 bus, 3 traffic lights, 7.7ms\n",
      "Speed: 1.9ms preprocess, 7.7ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n008-2018-08-30-15-16-55-0400__CAM_FRONT__1535657116112404.jpg: 384x640 3 cars, 1 bus, 3 traffic lights, 8.6ms\n",
      "Speed: 2.3ms preprocess, 8.6ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n008-2018-08-30-15-16-55-0400__CAM_FRONT__1535657116612404.jpg: 384x640 1 person, 2 cars, 1 truck, 4 traffic lights, 7.5ms\n",
      "Speed: 2.0ms preprocess, 7.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n008-2018-08-30-15-16-55-0400__CAM_FRONT__1535657117112404.jpg: 384x640 1 person, 2 cars, 3 traffic lights, 7.5ms\n",
      "Speed: 1.9ms preprocess, 7.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n008-2018-08-30-15-16-55-0400__CAM_FRONT__1535657117612404.jpg: 384x640 4 cars, 1 bus, 2 trucks, 2 traffic lights, 7.6ms\n",
      "Speed: 1.9ms preprocess, 7.6ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n008-2018-08-30-15-16-55-0400__CAM_FRONT__1535657118112404.jpg: 384x640 3 cars, 1 bus, 3 trucks, 3 traffic lights, 7.7ms\n",
      "Speed: 1.9ms preprocess, 7.7ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n008-2018-08-30-15-16-55-0400__CAM_FRONT__1535657118612404.jpg: 384x640 2 cars, 3 trucks, 1 traffic light, 7.7ms\n",
      "Speed: 1.9ms preprocess, 7.7ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n008-2018-08-30-15-16-55-0400__CAM_FRONT__1535657119112404.jpg: 384x640 2 persons, 1 truck, 3 traffic lights, 7.8ms\n",
      "Speed: 1.9ms preprocess, 7.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n008-2018-08-30-15-16-55-0400__CAM_FRONT__1535657119612404.jpg: 384x640 1 person, 1 car, 2 trucks, 2 traffic lights, 7.6ms\n",
      "Speed: 1.9ms preprocess, 7.6ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n008-2018-08-30-15-16-55-0400__CAM_FRONT__1535657120112404.jpg: 384x640 1 car, 3 trucks, 3 traffic lights, 7.7ms\n",
      "Speed: 2.3ms preprocess, 7.7ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n008-2018-08-30-15-16-55-0400__CAM_FRONT__1535657120612404.jpg: 384x640 1 person, 5 cars, 1 bus, 2 trucks, 3 traffic lights, 7.6ms\n",
      "Speed: 2.4ms preprocess, 7.6ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n008-2018-08-30-15-16-55-0400__CAM_FRONT__1535657121112404.jpg: 384x640 1 person, 2 cars, 3 traffic lights, 8.0ms\n",
      "Speed: 5.1ms preprocess, 8.0ms inference, 6.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n008-2018-08-30-15-16-55-0400__CAM_FRONT__1535657121612404.jpg: 384x640 3 persons, 3 cars, 4 traffic lights, 9.4ms\n",
      "Speed: 2.0ms preprocess, 9.4ms inference, 30.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n008-2018-08-30-15-16-55-0400__CAM_FRONT__1535657122112404.jpg: 384x640 3 persons, 6 cars, 4 traffic lights, 7.7ms\n",
      "Speed: 2.0ms preprocess, 7.7ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n008-2018-08-30-15-16-55-0400__CAM_FRONT__1535657122612404.jpg: 384x640 3 persons, 9 cars, 4 traffic lights, 9.5ms\n",
      "Speed: 4.8ms preprocess, 9.5ms inference, 23.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n008-2018-08-30-15-16-55-0400__CAM_FRONT__1535657123112404.jpg: 384x640 4 cars, 3 traffic lights, 9.5ms\n",
      "Speed: 4.7ms preprocess, 9.5ms inference, 4.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n008-2018-08-30-15-16-55-0400__CAM_FRONT__1535657123612404.jpg: 384x640 3 cars, 3 traffic lights, 9.4ms\n",
      "Speed: 6.0ms preprocess, 9.4ms inference, 30.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n008-2018-08-30-15-16-55-0400__CAM_FRONT__1535657124112404.jpg: 384x640 1 person, 5 cars, 3 traffic lights, 14.3ms\n",
      "Speed: 8.1ms preprocess, 14.3ms inference, 3.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n008-2018-08-30-15-16-55-0400__CAM_FRONT__1535657124612404.jpg: 384x640 3 persons, 6 cars, 4 traffic lights, 14.0ms\n",
      "Speed: 7.9ms preprocess, 14.0ms inference, 29.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n008-2018-08-30-15-16-55-0400__CAM_FRONT__1535657125112404.jpg: 384x640 3 persons, 2 cars, 2 traffic lights, 11.8ms\n",
      "Speed: 5.9ms preprocess, 11.8ms inference, 19.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n008-2018-08-30-15-16-55-0400__CAM_FRONT__1535657125612404.jpg: 384x640 1 person, 5 cars, 4 traffic lights, 8.2ms\n",
      "Speed: 5.4ms preprocess, 8.2ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n008-2018-08-30-15-16-55-0400__CAM_FRONT__1535657126112404.jpg: 384x640 1 person, 6 cars, 3 traffic lights, 9.4ms\n",
      "Speed: 4.7ms preprocess, 9.4ms inference, 30.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n008-2018-08-30-15-16-55-0400__CAM_FRONT__1535657126612404.jpg: 384x640 1 person, 6 cars, 3 traffic lights, 9.6ms\n",
      "Speed: 4.4ms preprocess, 9.6ms inference, 15.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n008-2018-08-30-15-16-55-0400__CAM_FRONT__1535657127112404.jpg: 384x640 1 person, 7 cars, 3 traffic lights, 9.5ms\n",
      "Speed: 3.6ms preprocess, 9.5ms inference, 4.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n008-2018-08-30-15-16-55-0400__CAM_FRONT__1535657127612404.jpg: 384x640 1 person, 2 cars, 2 traffic lights, 9.3ms\n",
      "Speed: 5.7ms preprocess, 9.3ms inference, 30.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n008-2018-08-30-15-16-55-0400__CAM_FRONT__1535657128112404.jpg: 384x640 3 persons, 3 cars, 1 truck, 3 traffic lights, 9.7ms\n",
      "Speed: 6.8ms preprocess, 9.7ms inference, 30.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-10-02-10-50-40+0800__CAM_FRONT__1538448744412460.jpg: 384x640 2 persons, 5 cars, 8.1ms\n",
      "Speed: 5.3ms preprocess, 8.1ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-10-02-10-50-40+0800__CAM_FRONT__1538448745012460.jpg: 384x640 5 cars, 9.4ms\n",
      "Speed: 4.2ms preprocess, 9.4ms inference, 30.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-10-02-10-50-40+0800__CAM_FRONT__1538448745512460.jpg: 384x640 2 cars, 9.7ms\n",
      "Speed: 5.7ms preprocess, 9.7ms inference, 15.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-10-02-10-50-40+0800__CAM_FRONT__1538448746012460.jpg: 384x640 3 cars, 8.0ms\n",
      "Speed: 2.2ms preprocess, 8.0ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-10-02-10-50-40+0800__CAM_FRONT__1538448746512460.jpg: 384x640 4 cars, 9.6ms\n",
      "Speed: 4.9ms preprocess, 9.6ms inference, 30.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-10-02-10-50-40+0800__CAM_FRONT__1538448747012460.jpg: 384x640 5 cars, 1 traffic light, 9.5ms\n",
      "Speed: 4.8ms preprocess, 9.5ms inference, 29.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-10-02-10-50-40+0800__CAM_FRONT__1538448747512460.jpg: 384x640 4 cars, 8.0ms\n",
      "Speed: 2.0ms preprocess, 8.0ms inference, 19.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-10-02-10-50-40+0800__CAM_FRONT__1538448748012460.jpg: 384x640 6 cars, 1 traffic light, 1 umbrella, 9.9ms\n",
      "Speed: 4.4ms preprocess, 9.9ms inference, 13.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-10-02-10-50-40+0800__CAM_FRONT__1538448748512460.jpg: 384x640 8 cars, 1 stop sign, 9.6ms\n",
      "Speed: 5.4ms preprocess, 9.6ms inference, 30.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-10-02-10-50-40+0800__CAM_FRONT__1538448749012460.jpg: 384x640 2 persons, 8 cars, 1 traffic light, 8.4ms\n",
      "Speed: 4.4ms preprocess, 8.4ms inference, 7.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-10-02-10-50-40+0800__CAM_FRONT__1538448749512460.jpg: 384x640 1 person, 5 cars, 9.5ms\n",
      "Speed: 3.1ms preprocess, 9.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-10-02-10-50-40+0800__CAM_FRONT__1538448750012460.jpg: 384x640 1 person, 5 cars, 1 umbrella, 9.2ms\n",
      "Speed: 3.0ms preprocess, 9.2ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-10-02-10-50-40+0800__CAM_FRONT__1538448750512460.jpg: 384x640 5 cars, 7.8ms\n",
      "Speed: 1.9ms preprocess, 7.8ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-10-02-10-50-40+0800__CAM_FRONT__1538448751012460.jpg: 384x640 7 cars, 8.0ms\n",
      "Speed: 1.9ms preprocess, 8.0ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-10-02-10-50-40+0800__CAM_FRONT__1538448751512460.jpg: 384x640 3 cars, 7.7ms\n",
      "Speed: 1.9ms preprocess, 7.7ms inference, 22.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-10-02-10-50-40+0800__CAM_FRONT__1538448752012460.jpg: 384x640 3 cars, 1 traffic light, 8.2ms\n",
      "Speed: 4.8ms preprocess, 8.2ms inference, 3.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-10-02-10-50-40+0800__CAM_FRONT__1538448752512460.jpg: 384x640 2 persons, 4 cars, 10.2ms\n",
      "Speed: 4.2ms preprocess, 10.2ms inference, 6.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-10-02-10-50-40+0800__CAM_FRONT__1538448753012460.jpg: 384x640 5 cars, 9.2ms\n",
      "Speed: 3.1ms preprocess, 9.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-10-02-10-50-40+0800__CAM_FRONT__1538448753512460.jpg: 384x640 4 cars, 10.5ms\n",
      "Speed: 2.6ms preprocess, 10.5ms inference, 3.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-10-02-10-50-40+0800__CAM_FRONT__1538448754012460.jpg: 384x640 8 cars, 9.9ms\n",
      "Speed: 2.4ms preprocess, 9.9ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-10-02-10-50-40+0800__CAM_FRONT__1538448754512460.jpg: 384x640 3 cars, 9.3ms\n",
      "Speed: 2.2ms preprocess, 9.3ms inference, 30.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-10-02-10-50-40+0800__CAM_FRONT__1538448755012460.jpg: 384x640 5 cars, 10.1ms\n",
      "Speed: 4.3ms preprocess, 10.1ms inference, 12.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-10-02-10-50-40+0800__CAM_FRONT__1538448755512460.jpg: 384x640 5 cars, 9.6ms\n",
      "Speed: 4.8ms preprocess, 9.6ms inference, 13.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-10-02-10-50-40+0800__CAM_FRONT__1538448756012460.jpg: 384x640 1 person, 4 cars, 8.8ms\n",
      "Speed: 2.2ms preprocess, 8.8ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-10-02-10-50-40+0800__CAM_FRONT__1538448756512460.jpg: 384x640 3 cars, 1 traffic light, 9.8ms\n",
      "Speed: 2.2ms preprocess, 9.8ms inference, 3.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-10-02-10-50-40+0800__CAM_FRONT__1538448757012460.jpg: 384x640 3 cars, 2 traffic lights, 9.1ms\n",
      "Speed: 2.2ms preprocess, 9.1ms inference, 3.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-10-02-10-50-40+0800__CAM_FRONT__1538448757512460.jpg: 384x640 2 cars, 1 traffic light, 11.8ms\n",
      "Speed: 2.5ms preprocess, 11.8ms inference, 30.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-10-02-10-50-40+0800__CAM_FRONT__1538448758012460.jpg: 384x640 2 cars, 1 traffic light, 8.9ms\n",
      "Speed: 2.5ms preprocess, 8.9ms inference, 5.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-10-02-10-50-40+0800__CAM_FRONT__1538448758512460.jpg: 384x640 2 cars, 9.4ms\n",
      "Speed: 2.3ms preprocess, 9.4ms inference, 3.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-10-02-10-50-40+0800__CAM_FRONT__1538448759012460.jpg: 384x640 2 cars, 8.9ms\n",
      "Speed: 2.1ms preprocess, 8.9ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-10-02-10-50-40+0800__CAM_FRONT__1538448759512460.jpg: 384x640 4 cars, 9.1ms\n",
      "Speed: 2.2ms preprocess, 9.1ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-10-02-10-50-40+0800__CAM_FRONT__1538448760012460.jpg: 384x640 4 cars, 10.8ms\n",
      "Speed: 2.5ms preprocess, 10.8ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-10-02-10-50-40+0800__CAM_FRONT__1538448760512460.jpg: 384x640 2 cars, 9.1ms\n",
      "Speed: 2.2ms preprocess, 9.1ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-10-02-10-50-40+0800__CAM_FRONT__1538448761012460.jpg: 384x640 2 cars, 9.2ms\n",
      "Speed: 2.2ms preprocess, 9.2ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-10-02-10-50-40+0800__CAM_FRONT__1538448761512460.jpg: 384x640 1 car, 9.2ms\n",
      "Speed: 2.4ms preprocess, 9.2ms inference, 3.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-10-02-10-50-40+0800__CAM_FRONT__1538448762012460.jpg: 384x640 1 person, 1 car, 9.1ms\n",
      "Speed: 2.3ms preprocess, 9.1ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-10-02-10-50-40+0800__CAM_FRONT__1538448762512460.jpg: 384x640 1 person, 2 cars, 10.7ms\n",
      "Speed: 2.3ms preprocess, 10.7ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-10-02-10-50-40+0800__CAM_FRONT__1538448763012460.jpg: 384x640 2 cars, 9.1ms\n",
      "Speed: 2.3ms preprocess, 9.1ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-10-02-10-50-40+0800__CAM_FRONT__1538448763512460.jpg: 384x640 2 cars, 9.1ms\n",
      "Speed: 2.3ms preprocess, 9.1ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-10-02-10-50-40+0800__CAM_FRONT__1538448764012460.jpg: 384x640 3 cars, 9.3ms\n",
      "Speed: 2.3ms preprocess, 9.3ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-10-08-15-36-50+0800__CAM_FRONT__1538984233512470.jpg: 384x640 5 persons, 9.9ms\n",
      "Speed: 2.3ms preprocess, 9.9ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-10-08-15-36-50+0800__CAM_FRONT__1538984234012472.jpg: 384x640 8 persons, 1 bicycle, 1 car, 9.3ms\n",
      "Speed: 2.3ms preprocess, 9.3ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-10-08-15-36-50+0800__CAM_FRONT__1538984234512460.jpg: 384x640 6 persons, 9.2ms\n",
      "Speed: 2.3ms preprocess, 9.2ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-10-08-15-36-50+0800__CAM_FRONT__1538984234912460.jpg: 384x640 7 persons, 9.8ms\n",
      "Speed: 3.1ms preprocess, 9.8ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-10-08-15-36-50+0800__CAM_FRONT__1538984235412460.jpg: 384x640 8 persons, 1 car, 9.2ms\n",
      "Speed: 2.3ms preprocess, 9.2ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-10-08-15-36-50+0800__CAM_FRONT__1538984235912460.jpg: 384x640 13 persons, 3 cars, 9.2ms\n",
      "Speed: 2.3ms preprocess, 9.2ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-10-08-15-36-50+0800__CAM_FRONT__1538984236412460.jpg: 384x640 10 persons, 3 cars, 9.3ms\n",
      "Speed: 2.3ms preprocess, 9.3ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-10-08-15-36-50+0800__CAM_FRONT__1538984236912460.jpg: 384x640 9 persons, 4 cars, 10.0ms\n",
      "Speed: 2.3ms preprocess, 10.0ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-10-08-15-36-50+0800__CAM_FRONT__1538984237412460.jpg: 384x640 6 persons, 4 cars, 9.4ms\n",
      "Speed: 2.5ms preprocess, 9.4ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-10-08-15-36-50+0800__CAM_FRONT__1538984237912460.jpg: 384x640 3 persons, 5 cars, 9.5ms\n",
      "Speed: 2.5ms preprocess, 9.5ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-10-08-15-36-50+0800__CAM_FRONT__1538984238412460.jpg: 384x640 2 persons, 1 bicycle, 4 cars, 1 motorcycle, 10.2ms\n",
      "Speed: 2.7ms preprocess, 10.2ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-10-08-15-36-50+0800__CAM_FRONT__1538984238912460.jpg: 384x640 3 persons, 3 cars, 9.2ms\n",
      "Speed: 2.3ms preprocess, 9.2ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-10-08-15-36-50+0800__CAM_FRONT__1538984239412460.jpg: 384x640 5 cars, 2 motorcycles, 9.2ms\n",
      "Speed: 2.3ms preprocess, 9.2ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-10-08-15-36-50+0800__CAM_FRONT__1538984239912460.jpg: 384x640 1 person, 4 cars, 9.4ms\n",
      "Speed: 2.4ms preprocess, 9.4ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-10-08-15-36-50+0800__CAM_FRONT__1538984240412460.jpg: 384x640 4 cars, 9.2ms\n",
      "Speed: 2.3ms preprocess, 9.2ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-10-08-15-36-50+0800__CAM_FRONT__1538984240912467.jpg: 384x640 5 cars, 9.4ms\n",
      "Speed: 2.3ms preprocess, 9.4ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-10-08-15-36-50+0800__CAM_FRONT__1538984241412460.jpg: 384x640 6 cars, 13.2ms\n",
      "Speed: 2.3ms preprocess, 13.2ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-10-08-15-36-50+0800__CAM_FRONT__1538984241912460.jpg: 384x640 9 cars, 9.0ms\n",
      "Speed: 2.3ms preprocess, 9.0ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-10-08-15-36-50+0800__CAM_FRONT__1538984242412460.jpg: 384x640 9 cars, 8.5ms\n",
      "Speed: 2.0ms preprocess, 8.5ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-10-08-15-36-50+0800__CAM_FRONT__1538984242912460.jpg: 384x640 11 cars, 8.0ms\n",
      "Speed: 1.9ms preprocess, 8.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-10-08-15-36-50+0800__CAM_FRONT__1538984243412460.jpg: 384x640 17 cars, 1 truck, 10.0ms\n",
      "Speed: 2.4ms preprocess, 10.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-10-08-15-36-50+0800__CAM_FRONT__1538984243912460.jpg: 384x640 15 cars, 1 truck, 8.4ms\n",
      "Speed: 2.1ms preprocess, 8.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-10-08-15-36-50+0800__CAM_FRONT__1538984244412460.jpg: 384x640 13 cars, 1 truck, 8.5ms\n",
      "Speed: 2.2ms preprocess, 8.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-10-08-15-36-50+0800__CAM_FRONT__1538984244912460.jpg: 384x640 12 cars, 1 truck, 8.5ms\n",
      "Speed: 2.2ms preprocess, 8.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-10-08-15-36-50+0800__CAM_FRONT__1538984245412460.jpg: 384x640 10 cars, 1 truck, 8.8ms\n",
      "Speed: 2.3ms preprocess, 8.8ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-10-08-15-36-50+0800__CAM_FRONT__1538984245912460.jpg: 384x640 9 cars, 1 truck, 8.4ms\n",
      "Speed: 2.2ms preprocess, 8.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-10-08-15-36-50+0800__CAM_FRONT__1538984246412460.jpg: 384x640 8 cars, 1 truck, 8.4ms\n",
      "Speed: 2.2ms preprocess, 8.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-10-08-15-36-50+0800__CAM_FRONT__1538984246912460.jpg: 384x640 10 cars, 1 truck, 8.2ms\n",
      "Speed: 2.2ms preprocess, 8.2ms inference, 30.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-10-08-15-36-50+0800__CAM_FRONT__1538984247412460.jpg: 384x640 10 cars, 9.4ms\n",
      "Speed: 3.0ms preprocess, 9.4ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-10-08-15-36-50+0800__CAM_FRONT__1538984247912460.jpg: 384x640 13 cars, 8.0ms\n",
      "Speed: 2.0ms preprocess, 8.0ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-10-08-15-36-50+0800__CAM_FRONT__1538984248412460.jpg: 384x640 9 cars, 8.3ms\n",
      "Speed: 2.1ms preprocess, 8.3ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-10-08-15-36-50+0800__CAM_FRONT__1538984248912460.jpg: 384x640 10 cars, 9.5ms\n",
      "Speed: 2.2ms preprocess, 9.5ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-10-08-15-36-50+0800__CAM_FRONT__1538984249412460.jpg: 384x640 9 cars, 8.5ms\n",
      "Speed: 2.2ms preprocess, 8.5ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-10-08-15-36-50+0800__CAM_FRONT__1538984249912460.jpg: 384x640 12 cars, 10.0ms\n",
      "Speed: 2.9ms preprocess, 10.0ms inference, 30.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-10-08-15-36-50+0800__CAM_FRONT__1538984250412460.jpg: 384x640 9 cars, 8.5ms\n",
      "Speed: 2.3ms preprocess, 8.5ms inference, 6.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-10-08-15-36-50+0800__CAM_FRONT__1538984250912460.jpg: 384x640 8 cars, 10.6ms\n",
      "Speed: 3.3ms preprocess, 10.6ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-10-08-15-36-50+0800__CAM_FRONT__1538984251412460.jpg: 384x640 6 cars, 10.0ms\n",
      "Speed: 3.3ms preprocess, 10.0ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-10-08-15-36-50+0800__CAM_FRONT__1538984251912460.jpg: 384x640 3 cars, 8.3ms\n",
      "Speed: 2.1ms preprocess, 8.3ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-10-08-15-36-50+0800__CAM_FRONT__1538984252412460.jpg: 384x640 3 cars, 8.5ms\n",
      "Speed: 2.1ms preprocess, 8.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-10-08-15-36-50+0800__CAM_FRONT__1538984252912460.jpg: 384x640 3 cars, 9.8ms\n",
      "Speed: 2.7ms preprocess, 9.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-10-08-15-36-50+0800__CAM_FRONT__1538984253412460.jpg: 384x640 1 person, 8.6ms\n",
      "Speed: 2.1ms preprocess, 8.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-11-21-19-38-26+0800__CAM_FRONT__1542800367912460.jpg: 384x640 4 cars, 9.0ms\n",
      "Speed: 2.1ms preprocess, 9.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-11-21-19-38-26+0800__CAM_FRONT__1542800368412460.jpg: 384x640 2 cars, 8.5ms\n",
      "Speed: 2.1ms preprocess, 8.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-11-21-19-38-26+0800__CAM_FRONT__1542800368912460.jpg: 384x640 9 cars, 8.8ms\n",
      "Speed: 2.2ms preprocess, 8.8ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-11-21-19-38-26+0800__CAM_FRONT__1542800369412460.jpg: 384x640 4 cars, 8.5ms\n",
      "Speed: 2.1ms preprocess, 8.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-11-21-19-38-26+0800__CAM_FRONT__1542800369912460.jpg: 384x640 5 cars, 8.5ms\n",
      "Speed: 2.1ms preprocess, 8.5ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-11-21-19-38-26+0800__CAM_FRONT__1542800370412460.jpg: 384x640 8 cars, 8.6ms\n",
      "Speed: 2.2ms preprocess, 8.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-11-21-19-38-26+0800__CAM_FRONT__1542800370912460.jpg: 384x640 9 cars, 8.9ms\n",
      "Speed: 2.0ms preprocess, 8.9ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-11-21-19-38-26+0800__CAM_FRONT__1542800371412460.jpg: 384x640 7 cars, 7.6ms\n",
      "Speed: 1.9ms preprocess, 7.6ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-11-21-19-38-26+0800__CAM_FRONT__1542800371912471.jpg: 384x640 7 cars, 7.4ms\n",
      "Speed: 1.9ms preprocess, 7.4ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-11-21-19-38-26+0800__CAM_FRONT__1542800372362460.jpg: 384x640 5 cars, 7.6ms\n",
      "Speed: 1.9ms preprocess, 7.6ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-11-21-19-38-26+0800__CAM_FRONT__1542800372912460.jpg: 384x640 4 cars, 7.7ms\n",
      "Speed: 1.9ms preprocess, 7.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-11-21-19-38-26+0800__CAM_FRONT__1542800373412460.jpg: 384x640 3 cars, 7.7ms\n",
      "Speed: 1.9ms preprocess, 7.7ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-11-21-19-38-26+0800__CAM_FRONT__1542800373912460.jpg: 384x640 7 cars, 8.0ms\n",
      "Speed: 2.0ms preprocess, 8.0ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-11-21-19-38-26+0800__CAM_FRONT__1542800374412460.jpg: 384x640 2 cars, 9.3ms\n",
      "Speed: 4.7ms preprocess, 9.3ms inference, 30.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-11-21-19-38-26+0800__CAM_FRONT__1542800374912460.jpg: 384x640 3 cars, 9.0ms\n",
      "Speed: 2.4ms preprocess, 9.0ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-11-21-19-38-26+0800__CAM_FRONT__1542800375412460.jpg: 384x640 4 cars, 10.3ms\n",
      "Speed: 3.3ms preprocess, 10.3ms inference, 4.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-11-21-19-38-26+0800__CAM_FRONT__1542800375912460.jpg: 384x640 5 cars, 10.4ms\n",
      "Speed: 3.5ms preprocess, 10.4ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-11-21-19-38-26+0800__CAM_FRONT__1542800376412496.jpg: 384x640 3 cars, 9.9ms\n",
      "Speed: 3.3ms preprocess, 9.9ms inference, 3.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-11-21-19-38-26+0800__CAM_FRONT__1542800376912460.jpg: 384x640 3 cars, 8.8ms\n",
      "Speed: 2.9ms preprocess, 8.8ms inference, 3.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-11-21-19-38-26+0800__CAM_FRONT__1542800377412460.jpg: 384x640 6 cars, 8.9ms\n",
      "Speed: 2.2ms preprocess, 8.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-11-21-19-38-26+0800__CAM_FRONT__1542800377862460.jpg: 384x640 4 cars, 8.9ms\n",
      "Speed: 2.3ms preprocess, 8.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-11-21-19-38-26+0800__CAM_FRONT__1542800378362460.jpg: 384x640 3 cars, 8.9ms\n",
      "Speed: 2.2ms preprocess, 8.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-11-21-19-38-26+0800__CAM_FRONT__1542800378862460.jpg: 384x640 1 car, 8.6ms\n",
      "Speed: 2.1ms preprocess, 8.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-11-21-19-38-26+0800__CAM_FRONT__1542800379362460.jpg: 384x640 2 cars, 8.9ms\n",
      "Speed: 2.2ms preprocess, 8.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-11-21-19-38-26+0800__CAM_FRONT__1542800379862460.jpg: 384x640 4 cars, 7.7ms\n",
      "Speed: 1.9ms preprocess, 7.7ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-11-21-19-38-26+0800__CAM_FRONT__1542800380362460.jpg: 384x640 2 cars, 7.7ms\n",
      "Speed: 1.9ms preprocess, 7.7ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-11-21-19-38-26+0800__CAM_FRONT__1542800380912460.jpg: 384x640 2 cars, 7.8ms\n",
      "Speed: 1.9ms preprocess, 7.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-11-21-19-38-26+0800__CAM_FRONT__1542800381412460.jpg: 384x640 5 cars, 8.2ms\n",
      "Speed: 2.1ms preprocess, 8.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-11-21-19-38-26+0800__CAM_FRONT__1542800381862460.jpg: 384x640 1 car, 9.1ms\n",
      "Speed: 2.2ms preprocess, 9.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-11-21-19-38-26+0800__CAM_FRONT__1542800382362460.jpg: 384x640 3 cars, 8.4ms\n",
      "Speed: 2.1ms preprocess, 8.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-11-21-19-38-26+0800__CAM_FRONT__1542800382862460.jpg: 384x640 4 cars, 8.4ms\n",
      "Speed: 2.1ms preprocess, 8.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-11-21-19-38-26+0800__CAM_FRONT__1542800383362460.jpg: 384x640 4 cars, 1 train, 8.4ms\n",
      "Speed: 2.1ms preprocess, 8.4ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-11-21-19-38-26+0800__CAM_FRONT__1542800383862460.jpg: 384x640 4 cars, 8.5ms\n",
      "Speed: 2.2ms preprocess, 8.5ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-11-21-19-38-26+0800__CAM_FRONT__1542800384362460.jpg: 384x640 3 cars, 9.6ms\n",
      "Speed: 2.6ms preprocess, 9.6ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-11-21-19-38-26+0800__CAM_FRONT__1542800384912460.jpg: 384x640 3 cars, 1 truck, 8.6ms\n",
      "Speed: 2.2ms preprocess, 8.6ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-11-21-19-38-26+0800__CAM_FRONT__1542800385412460.jpg: 384x640 3 cars, 8.3ms\n",
      "Speed: 2.2ms preprocess, 8.3ms inference, 25.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-11-21-19-38-26+0800__CAM_FRONT__1542800385912460.jpg: 384x640 6 cars, 8.0ms\n",
      "Speed: 2.0ms preprocess, 8.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-11-21-19-38-26+0800__CAM_FRONT__1542800386362460.jpg: 384x640 1 car, 9.5ms\n",
      "Speed: 2.1ms preprocess, 9.5ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-11-21-19-38-26+0800__CAM_FRONT__1542800386862460.jpg: 384x640 3 cars, 14.1ms\n",
      "Speed: 4.2ms preprocess, 14.1ms inference, 3.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-11-21-19-38-26+0800__CAM_FRONT__1542800387362460.jpg: 384x640 4 cars, 18.6ms\n",
      "Speed: 4.0ms preprocess, 18.6ms inference, 4.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-11-21-19-38-26+0800__CAM_FRONT__1542800387862460.jpg: 384x640 5 cars, 9.8ms\n",
      "Speed: 2.5ms preprocess, 9.8ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-11-21-19-38-26+0800__CAM_FRONT__1542800847912460.jpg: 384x640 1 person, 3 cars, 1 traffic light, 8.9ms\n",
      "Speed: 2.2ms preprocess, 8.9ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-11-21-19-38-26+0800__CAM_FRONT__1542800848412460.jpg: 384x640 3 persons, 4 cars, 1 truck, 1 traffic light, 9.0ms\n",
      "Speed: 2.2ms preprocess, 9.0ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-11-21-19-38-26+0800__CAM_FRONT__1542800848912460.jpg: 384x640 5 persons, 1 car, 1 traffic light, 8.4ms\n",
      "Speed: 2.2ms preprocess, 8.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-11-21-19-38-26+0800__CAM_FRONT__1542800849412460.jpg: 384x640 4 persons, 3 cars, 1 traffic light, 8.4ms\n",
      "Speed: 2.1ms preprocess, 8.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-11-21-19-38-26+0800__CAM_FRONT__1542800849912460.jpg: 384x640 5 persons, 3 cars, 8.8ms\n",
      "Speed: 2.2ms preprocess, 8.8ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-11-21-19-38-26+0800__CAM_FRONT__1542800850412460.jpg: 384x640 4 persons, 4 cars, 8.5ms\n",
      "Speed: 2.1ms preprocess, 8.5ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-11-21-19-38-26+0800__CAM_FRONT__1542800850912460.jpg: 384x640 4 persons, 2 cars, 8.5ms\n",
      "Speed: 2.2ms preprocess, 8.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-11-21-19-38-26+0800__CAM_FRONT__1542800851412460.jpg: 384x640 4 persons, 2 cars, 9.1ms\n",
      "Speed: 2.1ms preprocess, 9.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-11-21-19-38-26+0800__CAM_FRONT__1542800851912460.jpg: 384x640 5 persons, 2 cars, 8.6ms\n",
      "Speed: 2.1ms preprocess, 8.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-11-21-19-38-26+0800__CAM_FRONT__1542800852412460.jpg: 384x640 4 persons, 2 cars, 1 dog, 1 umbrella, 8.4ms\n",
      "Speed: 2.1ms preprocess, 8.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-11-21-19-38-26+0800__CAM_FRONT__1542800852912460.jpg: 384x640 4 persons, 2 cars, 1 truck, 1 umbrella, 8.7ms\n",
      "Speed: 2.3ms preprocess, 8.7ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-11-21-19-38-26+0800__CAM_FRONT__1542800853412460.jpg: 384x640 3 persons, 5 cars, 1 truck, 1 umbrella, 8.3ms\n",
      "Speed: 2.2ms preprocess, 8.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-11-21-19-38-26+0800__CAM_FRONT__1542800853912460.jpg: 384x640 2 persons, 6 cars, 1 truck, 1 umbrella, 8.3ms\n",
      "Speed: 2.2ms preprocess, 8.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-11-21-19-38-26+0800__CAM_FRONT__1542800854412460.jpg: 384x640 5 cars, 1 train, 8.3ms\n",
      "Speed: 2.1ms preprocess, 8.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-11-21-19-38-26+0800__CAM_FRONT__1542800854912460.jpg: 384x640 6 cars, 1 bus, 8.3ms\n",
      "Speed: 2.1ms preprocess, 8.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-11-21-19-38-26+0800__CAM_FRONT__1542800855412460.jpg: 384x640 3 cars, 1 traffic light, 8.3ms\n",
      "Speed: 2.2ms preprocess, 8.3ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-11-21-19-38-26+0800__CAM_FRONT__1542800855912460.jpg: 384x640 4 cars, 8.8ms\n",
      "Speed: 2.2ms preprocess, 8.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-11-21-19-38-26+0800__CAM_FRONT__1542800856412460.jpg: 384x640 3 cars, 8.6ms\n",
      "Speed: 2.2ms preprocess, 8.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-11-21-19-38-26+0800__CAM_FRONT__1542800856912460.jpg: 384x640 4 cars, 8.6ms\n",
      "Speed: 2.2ms preprocess, 8.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-11-21-19-38-26+0800__CAM_FRONT__1542800857412460.jpg: 384x640 5 cars, 8.8ms\n",
      "Speed: 2.2ms preprocess, 8.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-11-21-19-38-26+0800__CAM_FRONT__1542800857862460.jpg: 384x640 2 cars, 8.4ms\n",
      "Speed: 2.1ms preprocess, 8.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-11-21-19-38-26+0800__CAM_FRONT__1542800858362460.jpg: 384x640 2 cars, 9.4ms\n",
      "Speed: 2.1ms preprocess, 9.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-11-21-19-38-26+0800__CAM_FRONT__1542800858862460.jpg: 384x640 1 person, 2 cars, 1 truck, 8.5ms\n",
      "Speed: 2.1ms preprocess, 8.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-11-21-19-38-26+0800__CAM_FRONT__1542800859412460.jpg: 384x640 1 person, 2 cars, 8.5ms\n",
      "Speed: 2.1ms preprocess, 8.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-11-21-19-38-26+0800__CAM_FRONT__1542800859912460.jpg: 384x640 3 persons, 3 cars, 8.4ms\n",
      "Speed: 2.1ms preprocess, 8.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-11-21-19-38-26+0800__CAM_FRONT__1542800860412460.jpg: 384x640 2 persons, 1 car, 8.6ms\n",
      "Speed: 2.2ms preprocess, 8.6ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-11-21-19-38-26+0800__CAM_FRONT__1542800860912460.jpg: 384x640 2 persons, 4 cars, 9.1ms\n",
      "Speed: 2.3ms preprocess, 9.1ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-11-21-19-38-26+0800__CAM_FRONT__1542800861412460.jpg: 384x640 2 persons, 4 cars, 9.6ms\n",
      "Speed: 2.3ms preprocess, 9.6ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-11-21-19-38-26+0800__CAM_FRONT__1542800861912460.jpg: 384x640 2 persons, 7 cars, 1 truck, 9.0ms\n",
      "Speed: 2.2ms preprocess, 9.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-11-21-19-38-26+0800__CAM_FRONT__1542800862412460.jpg: 384x640 1 person, 2 cars, 8.8ms\n",
      "Speed: 2.2ms preprocess, 8.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-11-21-19-38-26+0800__CAM_FRONT__1542800862912460.jpg: 384x640 1 person, 1 car, 8.7ms\n",
      "Speed: 2.2ms preprocess, 8.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-11-21-19-38-26+0800__CAM_FRONT__1542800863412460.jpg: 384x640 3 cars, 8.8ms\n",
      "Speed: 2.2ms preprocess, 8.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-11-21-19-38-26+0800__CAM_FRONT__1542800863912460.jpg: 384x640 5 cars, 8.8ms\n",
      "Speed: 2.2ms preprocess, 8.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-11-21-19-38-26+0800__CAM_FRONT__1542800864412460.jpg: 384x640 5 cars, 1 truck, 9.5ms\n",
      "Speed: 2.6ms preprocess, 9.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-11-21-19-38-26+0800__CAM_FRONT__1542800864912460.jpg: 384x640 2 cars, 1 truck, 8.5ms\n",
      "Speed: 2.1ms preprocess, 8.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-11-21-19-38-26+0800__CAM_FRONT__1542800865412815.jpg: 384x640 2 cars, 1 truck, 8.5ms\n",
      "Speed: 2.2ms preprocess, 8.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-11-21-19-38-26+0800__CAM_FRONT__1542800865912460.jpg: 384x640 3 cars, 8.4ms\n",
      "Speed: 2.2ms preprocess, 8.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-11-21-19-38-26+0800__CAM_FRONT__1542800866412467.jpg: 384x640 2 cars, 1 truck, 8.4ms\n",
      "Speed: 2.1ms preprocess, 8.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-11-21-19-38-26+0800__CAM_FRONT__1542800866912460.jpg: 384x640 1 car, 1 truck, 10.8ms\n",
      "Speed: 2.1ms preprocess, 10.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-11-21-19-38-26+0800__CAM_FRONT__1542800867412460.jpg: 384x640 1 car, 1 truck, 8.7ms\n",
      "Speed: 2.2ms preprocess, 8.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-11-21-19-38-26+0800__CAM_FRONT__1542800987912460.jpg: 384x640 6 cars, 8.5ms\n",
      "Speed: 2.1ms preprocess, 8.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-11-21-19-38-26+0800__CAM_FRONT__1542800988412460.jpg: 384x640 3 cars, 8.5ms\n",
      "Speed: 2.2ms preprocess, 8.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-11-21-19-38-26+0800__CAM_FRONT__1542800988912460.jpg: 384x640 1 person, 4 cars, 8.5ms\n",
      "Speed: 2.2ms preprocess, 8.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-11-21-19-38-26+0800__CAM_FRONT__1542800989412460.jpg: 384x640 7 cars, 8.5ms\n",
      "Speed: 2.2ms preprocess, 8.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-11-21-19-38-26+0800__CAM_FRONT__1542800989912460.jpg: 384x640 5 cars, 9.8ms\n",
      "Speed: 3.7ms preprocess, 9.8ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-11-21-19-38-26+0800__CAM_FRONT__1542800990412460.jpg: 384x640 1 person, 2 cars, 1 traffic light, 8.6ms\n",
      "Speed: 2.1ms preprocess, 8.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-11-21-19-38-26+0800__CAM_FRONT__1542800990912460.jpg: 384x640 1 person, 2 cars, 8.5ms\n",
      "Speed: 2.1ms preprocess, 8.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-11-21-19-38-26+0800__CAM_FRONT__1542800991412460.jpg: 384x640 1 person, 5 cars, 1 traffic light, 8.6ms\n",
      "Speed: 2.1ms preprocess, 8.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-11-21-19-38-26+0800__CAM_FRONT__1542800991912460.jpg: 384x640 7 cars, 8.5ms\n",
      "Speed: 2.1ms preprocess, 8.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-11-21-19-38-26+0800__CAM_FRONT__1542800992412460.jpg: 384x640 1 person, 5 cars, 8.3ms\n",
      "Speed: 2.1ms preprocess, 8.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-11-21-19-38-26+0800__CAM_FRONT__1542800992862460.jpg: 384x640 1 person, 4 cars, 8.8ms\n",
      "Speed: 2.2ms preprocess, 8.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-11-21-19-38-26+0800__CAM_FRONT__1542800993362460.jpg: 384x640 2 persons, 4 cars, 8.5ms\n",
      "Speed: 2.1ms preprocess, 8.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-11-21-19-38-26+0800__CAM_FRONT__1542800993862460.jpg: 384x640 1 person, 4 cars, 8.5ms\n",
      "Speed: 2.2ms preprocess, 8.5ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-11-21-19-38-26+0800__CAM_FRONT__1542800994362460.jpg: 384x640 5 cars, 8.5ms\n",
      "Speed: 2.2ms preprocess, 8.5ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-11-21-19-38-26+0800__CAM_FRONT__1542800994862460.jpg: 384x640 1 person, 3 cars, 1 truck, 8.7ms\n",
      "Speed: 2.2ms preprocess, 8.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-11-21-19-38-26+0800__CAM_FRONT__1542800995362460.jpg: 384x640 4 cars, 8.5ms\n",
      "Speed: 2.2ms preprocess, 8.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-11-21-19-38-26+0800__CAM_FRONT__1542800995862460.jpg: 384x640 1 person, 2 cars, 8.5ms\n",
      "Speed: 2.1ms preprocess, 8.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-11-21-19-38-26+0800__CAM_FRONT__1542800996412460.jpg: 384x640 3 cars, 8.9ms\n",
      "Speed: 2.3ms preprocess, 8.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-11-21-19-38-26+0800__CAM_FRONT__1542800996912460.jpg: 384x640 6 cars, 8.5ms\n",
      "Speed: 2.1ms preprocess, 8.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-11-21-19-38-26+0800__CAM_FRONT__1542800997412460.jpg: 384x640 4 cars, 8.4ms\n",
      "Speed: 2.1ms preprocess, 8.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-11-21-19-38-26+0800__CAM_FRONT__1542800997912460.jpg: 384x640 5 cars, 8.4ms\n",
      "Speed: 2.1ms preprocess, 8.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-11-21-19-38-26+0800__CAM_FRONT__1542800998362460.jpg: 384x640 6 cars, 8.4ms\n",
      "Speed: 2.1ms preprocess, 8.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-11-21-19-38-26+0800__CAM_FRONT__1542800998912460.jpg: 384x640 1 car, 8.5ms\n",
      "Speed: 2.2ms preprocess, 8.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-11-21-19-38-26+0800__CAM_FRONT__1542800999412460.jpg: 384x640 (no detections), 8.3ms\n",
      "Speed: 2.1ms preprocess, 8.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-11-21-19-38-26+0800__CAM_FRONT__1542800999912460.jpg: 384x640 1 person, 8.6ms\n",
      "Speed: 2.2ms preprocess, 8.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-11-21-19-38-26+0800__CAM_FRONT__1542801000412460.jpg: 384x640 2 persons, 1 car, 8.3ms\n",
      "Speed: 2.1ms preprocess, 8.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-11-21-19-38-26+0800__CAM_FRONT__1542801000912460.jpg: 384x640 2 persons, 8.4ms\n",
      "Speed: 2.1ms preprocess, 8.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-11-21-19-38-26+0800__CAM_FRONT__1542801001412460.jpg: 384x640 (no detections), 8.3ms\n",
      "Speed: 2.1ms preprocess, 8.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-11-21-19-38-26+0800__CAM_FRONT__1542801001912460.jpg: 384x640 1 person, 1 skateboard, 8.3ms\n",
      "Speed: 2.2ms preprocess, 8.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-11-21-19-38-26+0800__CAM_FRONT__1542801002412460.jpg: 384x640 (no detections), 8.3ms\n",
      "Speed: 2.1ms preprocess, 8.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-11-21-19-38-26+0800__CAM_FRONT__1542801002912460.jpg: 384x640 1 car, 8.3ms\n",
      "Speed: 2.1ms preprocess, 8.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-11-21-19-38-26+0800__CAM_FRONT__1542801003412460.jpg: 384x640 1 car, 8.8ms\n",
      "Speed: 2.2ms preprocess, 8.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-11-21-19-38-26+0800__CAM_FRONT__1542801003912460.jpg: 384x640 2 cars, 8.4ms\n",
      "Speed: 2.2ms preprocess, 8.4ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-11-21-19-38-26+0800__CAM_FRONT__1542801004412460.jpg: 384x640 3 cars, 8.3ms\n",
      "Speed: 2.2ms preprocess, 8.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-11-21-19-38-26+0800__CAM_FRONT__1542801004912460.jpg: 384x640 2 cars, 8.3ms\n",
      "Speed: 2.1ms preprocess, 8.3ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-11-21-19-38-26+0800__CAM_FRONT__1542801005412460.jpg: 384x640 2 cars, 8.2ms\n",
      "Speed: 2.1ms preprocess, 8.2ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-11-21-19-38-26+0800__CAM_FRONT__1542801005912460.jpg: 384x640 2 cars, 8.3ms\n",
      "Speed: 2.1ms preprocess, 8.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-11-21-19-38-26+0800__CAM_FRONT__1542801006412460.jpg: 384x640 2 cars, 8.3ms\n",
      "Speed: 2.1ms preprocess, 8.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-11-21-19-38-26+0800__CAM_FRONT__1542801006912460.jpg: 384x640 1 car, 9.1ms\n",
      "Speed: 2.3ms preprocess, 9.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 /home/rahman2/CMPE691_CV/Project/data/sets/nuscenes/samples/CAM_FRONT/n015-2018-11-21-19-38-26+0800__CAM_FRONT__1542801007412469.jpg: 384x640 3 cars, 8.9ms\n",
      "Speed: 2.1ms preprocess, 8.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    }
   ],
   "source": [
    "image_dataset_xywh = []\n",
    "for datapath in front_cam_images_datapath:\n",
    "    results = model(datapath)  # predict on an image\n",
    "    for result in results:\n",
    "        image_xywh = [[float(item[0]),float(item[1]),float(item[2]),float(item[3])] for item in result.boxes.xywh]\n",
    "        image_dataset_xywh.append(image_xywh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "926fbb27-1666-4680-9520-ca98faeda7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# radar_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90941ef4-3383-4515-adf8-e03f0ae08fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_dataset_xywh[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d8b736-da14-4d7e-a6be-356c4b7c0d94",
   "metadata": {},
   "source": [
    "#### Estimate depth using MiDaS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f6749100-7c08-44ce-aef6-532041d54bc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/rahman2/.cache/torch/hub/intel-isl_MiDaS_master\n",
      "/home/rahman2/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "model_type = \"DPT_Large\"     # MiDaS v3 - Large     (highest accuracy, slowest inference speed)\n",
    "#model_type = \"DPT_Hybrid\"   # MiDaS v3 - Hybrid    (medium accuracy, medium inference speed)\n",
    "#model_type = \"MiDaS_small\"  # MiDaS v2.1 - Small   (lowest accuracy, highest inference speed)\n",
    "\n",
    "midas = torch.hub.load(\"intel-isl/MiDaS\", model_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56b652e5-33b9-495a-8f7e-af64aff4b22f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DPTDepthModel(\n",
       "  (pretrained): Module(\n",
       "    (model): VisionTransformer(\n",
       "      (patch_embed): PatchEmbed(\n",
       "        (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))\n",
       "        (norm): Identity()\n",
       "      )\n",
       "      (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "      (patch_drop): Identity()\n",
       "      (norm_pre): Identity()\n",
       "      (blocks): Sequential(\n",
       "        (0): Block(\n",
       "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (1): Block(\n",
       "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (2): Block(\n",
       "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (3): Block(\n",
       "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (4): Block(\n",
       "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (5): Block(\n",
       "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (6): Block(\n",
       "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (7): Block(\n",
       "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (8): Block(\n",
       "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (9): Block(\n",
       "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (10): Block(\n",
       "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (11): Block(\n",
       "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (12): Block(\n",
       "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (13): Block(\n",
       "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (14): Block(\n",
       "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (15): Block(\n",
       "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (16): Block(\n",
       "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (17): Block(\n",
       "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (18): Block(\n",
       "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (19): Block(\n",
       "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (20): Block(\n",
       "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (21): Block(\n",
       "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (22): Block(\n",
       "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (23): Block(\n",
       "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (fc_norm): Identity()\n",
       "      (head_drop): Dropout(p=0.0, inplace=False)\n",
       "      (head): Linear(in_features=1024, out_features=1000, bias=True)\n",
       "    )\n",
       "    (act_postprocess1): Sequential(\n",
       "      (0): ProjectReadout(\n",
       "        (project): Sequential(\n",
       "          (0): Linear(in_features=2048, out_features=1024, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "        )\n",
       "      )\n",
       "      (1): Transpose()\n",
       "      (2): Unflatten(dim=2, unflattened_size=torch.Size([24, 24]))\n",
       "      (3): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (4): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(4, 4))\n",
       "    )\n",
       "    (act_postprocess2): Sequential(\n",
       "      (0): ProjectReadout(\n",
       "        (project): Sequential(\n",
       "          (0): Linear(in_features=2048, out_features=1024, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "        )\n",
       "      )\n",
       "      (1): Transpose()\n",
       "      (2): Unflatten(dim=2, unflattened_size=torch.Size([24, 24]))\n",
       "      (3): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (4): ConvTranspose2d(512, 512, kernel_size=(2, 2), stride=(2, 2))\n",
       "    )\n",
       "    (act_postprocess3): Sequential(\n",
       "      (0): ProjectReadout(\n",
       "        (project): Sequential(\n",
       "          (0): Linear(in_features=2048, out_features=1024, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "        )\n",
       "      )\n",
       "      (1): Transpose()\n",
       "      (2): Unflatten(dim=2, unflattened_size=torch.Size([24, 24]))\n",
       "      (3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (act_postprocess4): Sequential(\n",
       "      (0): ProjectReadout(\n",
       "        (project): Sequential(\n",
       "          (0): Linear(in_features=2048, out_features=1024, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "        )\n",
       "      )\n",
       "      (1): Transpose()\n",
       "      (2): Unflatten(dim=2, unflattened_size=torch.Size([24, 24]))\n",
       "      (3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (4): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (scratch): Module(\n",
       "    (layer1_rn): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (layer2_rn): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (layer3_rn): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (layer4_rn): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (refinenet1): FeatureFusionBlock_custom(\n",
       "      (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (resConfUnit1): ResidualConvUnit_custom(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (activation): ReLU()\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (resConfUnit2): ResidualConvUnit_custom(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (activation): ReLU()\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (skip_add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "    (refinenet2): FeatureFusionBlock_custom(\n",
       "      (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (resConfUnit1): ResidualConvUnit_custom(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (activation): ReLU()\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (resConfUnit2): ResidualConvUnit_custom(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (activation): ReLU()\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (skip_add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "    (refinenet3): FeatureFusionBlock_custom(\n",
       "      (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (resConfUnit1): ResidualConvUnit_custom(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (activation): ReLU()\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (resConfUnit2): ResidualConvUnit_custom(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (activation): ReLU()\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (skip_add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "    (refinenet4): FeatureFusionBlock_custom(\n",
       "      (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (resConfUnit1): ResidualConvUnit_custom(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (activation): ReLU()\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (resConfUnit2): ResidualConvUnit_custom(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (activation): ReLU()\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (skip_add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "    (output_conv): Sequential(\n",
       "      (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): Interpolate()\n",
       "      (2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): Conv2d(32, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (5): ReLU(inplace=True)\n",
       "      (6): Identity()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "midas.to(device)\n",
    "midas.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a8f8dec2-fcd4-40e4-b596-f83777ad5b30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/rahman2/.cache/torch/hub/intel-isl_MiDaS_master\n"
     ]
    }
   ],
   "source": [
    "midas_transforms = torch.hub.load(\"intel-isl/MiDaS\", \"transforms\")\n",
    "\n",
    "if model_type == \"DPT_Large\" or model_type == \"DPT_Hybrid\":\n",
    "    transform = midas_transforms.dpt_transform\n",
    "else:\n",
    "    transform = midas_transforms.small_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6b60f298-f18a-44c4-a08c-5401eb29bedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i,data_path in enumerate(front_cam_images_datapath):\n",
    "midas_outputs = []\n",
    "# for i in range(5):\n",
    "for i in range(len(front_cam_images_datapath)):\n",
    "    data_path = front_cam_images_datapath[i]\n",
    "    # print(data_path)\n",
    "    img = cv2.imread(data_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    input_batch = transform(img).to(device)\n",
    "    with torch.no_grad():\n",
    "        prediction = midas(input_batch)\n",
    "        prediction = torch.nn.functional.interpolate(\n",
    "            prediction.unsqueeze(1),\n",
    "            size=img.shape[:2],\n",
    "            mode=\"bicubic\",\n",
    "            align_corners=False,\n",
    "        ).squeeze()\n",
    "\n",
    "    output = prediction.cpu().numpy()\n",
    "    midas_outputs.append(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3f69678f-1557-485f-b984-8e6307386ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# depth_values = []\n",
    "num_bboxes = []\n",
    "image_dataset_xywhd = []\n",
    "# for i in range(5):\n",
    "for i in range(len(front_cam_images_datapath)):\n",
    "    depth_image = midas_outputs[i]\n",
    "    temp_image_xywh = image_dataset_xywh[i]\n",
    "    image_xywhd = []\n",
    "    num_bboxes.append(len(temp_image_xywh))\n",
    "    for xy in temp_image_xywh:\n",
    "        # print(xy,i)\n",
    "        center_x = int(xy[0])\n",
    "        center_y = int(xy[1])\n",
    "        depth = depth_image[center_y,center_x]\n",
    "        image_xywhd.append([xy[0],xy[1],xy[2],xy[3],depth])\n",
    "    image_dataset_xywhd.append(image_xywhd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bcbb4385-9fb0-4a63-b2af-fdb3c4408dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c3cc8adb-8647-4f85-83b3-9bf965667372",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Sort the bboxes w.r.t depth\n",
    "image_dataset_xywh_sorted = []\n",
    "for i in range(len(front_cam_images_datapath)):\n",
    "    image_xywhd = image_dataset_xywhd[i]\n",
    "    # print(image_xywhd)\n",
    "    image_xywhd_sorted = sorted(image_xywhd,key=lambda x:x[4])\n",
    "    # print(image_xywhd_sorted)\n",
    "    ## Remove the depth entry\n",
    "    image_xywh_sorted = []\n",
    "    for item in image_xywhd_sorted:\n",
    "        # image_xywh_sorted.append(item[:-1])\n",
    "        image_dataset_xywh_sorted.append(item[:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc65849-39dd-49c8-9547-fb6a463420f1",
   "metadata": {},
   "source": [
    "#### Cluster the radar point clouds with K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e95bbb0f-e8ab-45e2-9678-7ccb5b2747ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(radar_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4431799e-63bf-46c5-9d83-3549be955b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6f0f72cb-1ea6-4f69-b5e3-4617cbb219a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "radar_dataset_xy_kmeans = []\n",
    "# for i in range(3):\n",
    "for i in range(len(front_cam_images_datapath)):\n",
    "    if num_bboxes[i]>0:\n",
    "        n_clusters = num_bboxes[i]\n",
    "    # else:\n",
    "    #     n_clusters = 1\n",
    "        radar_xy = radar_dataset[i]\n",
    "        kmeans = KMeans(n_clusters=n_clusters, random_state=0) \n",
    "        kmeans.fit(radar_xy)\n",
    "        kmeans_radar_xy = kmeans.cluster_centers_\n",
    "        radar_dataset_xy_kmeans.append(kmeans_radar_xy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a35a4c11-d88d-4912-85c1-bb7150550d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compute radar distance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f23fef06-adbe-4688-a7e9-68fe17324dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "radar_dataset_xyd_kmeans = []\n",
    "# for i in range(3):\n",
    "for i in range(len(radar_dataset_xy_kmeans)):\n",
    "    kmeans_radar_xy = radar_dataset_xy_kmeans[i]\n",
    "    kmeans_radar_xyd = []\n",
    "    for xy in kmeans_radar_xy:\n",
    "        # print(xy)\n",
    "        dist = np.sqrt(xy[0]**2+xy[1]**2)\n",
    "        kmeans_radar_xyd.append([xy[0],xy[1],dist])\n",
    "    radar_dataset_xyd_kmeans.append(kmeans_radar_xyd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1ea3d00c-3ebb-45df-bc7e-4a603c2dc938",
   "metadata": {},
   "outputs": [],
   "source": [
    "# radar_dataset_xyd_kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "67733946-b7bb-488f-95e2-606e8104167d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sort radar point clouds w.r.t distance\n",
    "radar_dataset_xy_sorted = []\n",
    "# for i in range(3):\n",
    "for i in range(len(radar_dataset_xyd_kmeans)):\n",
    "    radar_xyd = radar_dataset_xyd_kmeans[i]\n",
    "    radar_xyd_sorted = sorted(radar_xyd,key=lambda x:x[2],reverse=True)\n",
    "    ## Remove the depth entry\n",
    "    radar_xy_sorted = []\n",
    "    for item in radar_xyd_sorted:\n",
    "        # radar_xy_sorted.append(item[:-1])\n",
    "        radar_dataset_xy_sorted.append(item[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cc445f0a-f677-4776-a1da-1a4f29c0f6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# radar_dataset_xy_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12808b9f-ff9a-4696-a3cc-aedb9100faf6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2388a957-7a5d-44b1-9aa4-52e46c98d6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "\n",
    "radar_data = np.array(radar_dataset_xy_sorted)\n",
    "image_data = np.array(image_dataset_xywh_sorted)\n",
    "\n",
    "radar_data_tensor = torch.Tensor(radar_data) # transform to torch tensor\n",
    "image_data_tensor = torch.Tensor(image_data)\n",
    "\n",
    "dataset = TensorDataset(radar_data_tensor,image_data_tensor) # create your datset\n",
    "# dataloader = DataLoader(dataset) # create your dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9833e60f-0ba6-473d-9c92-4838149fd030",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "70a372fd-ded9-400e-8423-844059420c1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3215, 2572, 643)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader = DataLoader(train_dataset)\n",
    "test_loader = DataLoader(test_dataset)\n",
    "len(dataset),len(train_loader),len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f76726b-b6a7-4ba8-a01c-71f295a705d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c18faa-52df-4608-a13a-e9188daaae22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f4f714-db64-4999-a539-19e141ff2f13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2331ea41-1eda-49bb-8f26-d0d9b84372bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AE(torch.nn.Module):\n",
    "\tdef __init__(self):\n",
    "\t\tsuper().__init__()\n",
    "\t\t\n",
    "\t\t# Building an linear encoder with Linear\n",
    "\t\t# layer followed by Relu activation function\n",
    "\t\t# 784 ==> 9\n",
    "\t\tself.encoder = torch.nn.Sequential(\n",
    "\t\t\ttorch.nn.Linear(2,1),\n",
    "\t\t\ttorch.nn.ReLU(),\n",
    "\t\t\ttorch.nn.Linear(1,16),\n",
    "\t\t\ttorch.nn.ReLU(),\n",
    "\t\t\ttorch.nn.Linear(16,32),\n",
    "\t\t\ttorch.nn.ReLU(),\n",
    "\t\t\ttorch.nn.Linear(32,64),\n",
    "\t\t\ttorch.nn.ReLU(),\n",
    "\t\t\ttorch.nn.Linear(64,80)\n",
    "\t\t)\n",
    "\t\t\n",
    "\t\t# Building an linear decoder with Linear\n",
    "\t\t# layer followed by Relu activation function\n",
    "\t\t# The Sigmoid activation function\n",
    "\t\t# outputs the value between 0 and 1\n",
    "\t\t# 9 ==> 784\n",
    "\t\tself.decoder = torch.nn.Sequential(\n",
    "\t\t\ttorch.nn.Linear(80,64),\n",
    "\t\t\ttorch.nn.ReLU(),\n",
    "\t\t\ttorch.nn.Linear(64,32),\n",
    "\t\t\ttorch.nn.ReLU(),\n",
    "\t\t\ttorch.nn.Linear(32,16),\n",
    "\t\t\ttorch.nn.ReLU(),\n",
    "\t\t\ttorch.nn.Linear(16,2),\n",
    "\t\t\ttorch.nn.ReLU(),\n",
    "\t\t\ttorch.nn.Linear(2,4)\n",
    "\t\t\t# torch.nn.Sigmoid()\n",
    "\t\t)\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tencoded = self.encoder(x)\n",
    "\t\tdecoded = self.decoder(encoded)\n",
    "\t\treturn decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ec44902d-4851-418b-bc0d-4551a86b7d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Initialization\n",
    "model = AE()\n",
    "\n",
    "# Validation using MSE Loss function\n",
    "# loss_function = torch.nn.MSELoss()\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Using an Adam Optimizer with lr = 0.1\n",
    "optimizer = torch.optim.Adam(model.parameters(),\n",
    "\t\t\t\t\t\t\tlr = 1e-1,\n",
    "\t\t\t\t\t\t\tweight_decay = 1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3899c0ee-141f-4b37-bb11-c52159e63188",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4a54b3b8-b0ba-4cc1-8efd-5b217d547af3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0 Frames:2572 Training loss:1086.760009765625\n",
      "Epoch:0 Frames:643 Testing loss:1912.052490234375\n",
      "Epoch:1 Frames:2572 Training loss:1086.760009765625\n",
      "Epoch:1 Frames:643 Testing loss:1912.052490234375\n",
      "Epoch:2 Frames:2572 Training loss:1086.760009765625\n",
      "Epoch:2 Frames:643 Testing loss:1912.052490234375\n",
      "Epoch:3 Frames:2572 Training loss:1086.760009765625\n",
      "Epoch:3 Frames:643 Testing loss:1912.052490234375\n",
      "Epoch:4 Frames:2572 Training loss:1086.760009765625\n",
      "Epoch:4 Frames:643 Testing loss:1912.052490234375\n",
      "Epoch:5 Frames:2572 Training loss:1086.760009765625\n",
      "Epoch:5 Frames:643 Testing loss:1912.052490234375\n",
      "Epoch:6 Frames:2572 Training loss:1086.760009765625\n",
      "Epoch:6 Frames:643 Testing loss:1912.052490234375\n",
      "Epoch:7 Frames:2572 Training loss:1086.760009765625\n",
      "Epoch:7 Frames:643 Testing loss:1912.052490234375\n",
      "Epoch:8 Frames:2572 Training loss:1086.760009765625\n",
      "Epoch:8 Frames:643 Testing loss:1912.052490234375\n",
      "Epoch:9 Frames:2572 Training loss:1086.760009765625\n",
      "Epoch:9 Frames:643 Testing loss:1912.052490234375\n",
      "Epoch:10 Frames:2572 Training loss:1086.760009765625\n",
      "Epoch:10 Frames:643 Testing loss:1912.052490234375\n",
      "Epoch:11 Frames:2572 Training loss:1086.760009765625\n",
      "Epoch:11 Frames:643 Testing loss:1912.052490234375\n",
      "Epoch:12 Frames:2572 Training loss:1086.760009765625\n",
      "Epoch:12 Frames:643 Testing loss:1912.052490234375\n",
      "Epoch:13 Frames:2572 Training loss:1086.760009765625\n",
      "Epoch:13 Frames:643 Testing loss:1912.052490234375\n",
      "Epoch:14 Frames:2572 Training loss:1086.760009765625\n",
      "Epoch:14 Frames:643 Testing loss:1912.052490234375\n",
      "Epoch:15 Frames:2572 Training loss:1086.760009765625\n",
      "Epoch:15 Frames:643 Testing loss:1912.052490234375\n",
      "Epoch:16 Frames:2572 Training loss:1086.760009765625\n",
      "Epoch:16 Frames:643 Testing loss:1912.052490234375\n",
      "Epoch:17 Frames:2572 Training loss:1086.760009765625\n",
      "Epoch:17 Frames:643 Testing loss:1912.052490234375\n",
      "Epoch:18 Frames:2572 Training loss:1086.760009765625\n",
      "Epoch:18 Frames:643 Testing loss:1912.052490234375\n",
      "Epoch:19 Frames:2572 Training loss:1086.760009765625\n",
      "Epoch:19 Frames:643 Testing loss:1912.052490234375\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "for epoch in range(epochs):\n",
    "    ## Train model\n",
    "    i = 0\n",
    "    train_loss = 0\n",
    "    model.train()\n",
    "    for (radar_xy,image_xy) in train_loader:\n",
    "        # Output of Autoencoder\n",
    "        reconstructed = model(radar_xy)\n",
    "        # Calculating the loss function\n",
    "        loss = loss_function(reconstructed, image_xy)\n",
    "        # The gradients are set to zero,\n",
    "        # the gradient is computed and stored.\n",
    "        # .step() performs parameter update\n",
    "        loss.backward()\n",
    "        optimizer.zero_grad()\n",
    "        optimizer.step()\n",
    "        \n",
    "        i += 1\n",
    "        train_loss += loss.detach().numpy()\n",
    "    print(f\"Epoch:{epoch} Frames:{i} Training loss:{loss}\")\n",
    "    train_losses.append(train_loss)\n",
    "    ## Test model\n",
    "    model.eval()\n",
    "    i = 0\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for (radar_xy,image_xy) in test_loader:\n",
    "            reconstructed = model(radar_xy)\n",
    "            loss = loss_function(reconstructed, image_xy)\n",
    "            i += 1\n",
    "            test_loss += loss.detach().numpy()\n",
    "        print(f\"Epoch:{epoch} Frames:{i} Testing loss:{loss}\")\n",
    "        test_losses.append(test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2137f1-d13d-4b20-89e1-a1a1bb3b47b1",
   "metadata": {},
   "source": [
    "#### Plot training and testing loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "53f7e5eb-a67d-43df-9aff-4caacd05e5ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(train_losses)\n",
    "plt.grid()\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Training loss (Cross entropy)\")\n",
    "plt.xticks(np.arange(epochs))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6720b2c7-e636-4e97-b007-6668c60c59f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(test_losses)\n",
    "plt.grid()\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Testing loss (Cross entropy)\")\n",
    "plt.xticks(np.arange(epochs))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c8c7d1-82bb-40aa-98bb-30b9b55f23e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be2a35f-445e-43d2-976d-af9d3c41b6e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b24b34a-8d18-4f8f-9adb-597c38bfd65f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf5a995-7b77-40a5-ad0b-ed50f3534db4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff091b97-cb48-4965-b273-c49ba97a59ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791b821b-c946-44fd-a52d-c32c131d5d55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
